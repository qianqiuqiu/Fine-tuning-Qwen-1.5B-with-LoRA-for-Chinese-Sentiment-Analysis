# ğŸ¯ Qwen-1.5B ä¸­æ–‡æƒ…æ„Ÿåˆ†æå¾®è°ƒé¡¹ç›®

åŸºäº LoRA/QLoRA æŠ€æœ¯å¾®è°ƒ Qwen2.5-1.5B æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆçš„ä¸­æ–‡æƒ…æ„Ÿåˆ†æä»»åŠ¡ã€‚

## ğŸ“‹ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç«¯åˆ°ç«¯ä¸­æ–‡æƒ…æ„Ÿåˆ†æè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ã€‚é¡¹ç›®ç‰¹ç‚¹ï¼š

- **é«˜æ•ˆå¾®è°ƒ**ï¼šä½¿ç”¨ LoRA/QLoRA æŠ€æœ¯ï¼Œä»…éœ€å¾®è°ƒå°‘é‡å‚æ•°ï¼ˆçº¦ 1% æ¨¡å‹å‚æ•°ï¼‰
- **ä½æ˜¾å­˜å ç”¨**ï¼šQLoRA æ¨¡å¼ä¸‹ä»…éœ€ 4GB æ˜¾å­˜å³å¯è®­ç»ƒ
- **å®Œæ•´å·¥ä½œæµ**ï¼šæ¶µç›–æ•°æ®å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€æ¨ç†å…¨æµç¨‹
- **è¯¦ç»†è¯„ä¼°**ï¼šæä¾›å¤šç»´åº¦æ¨¡å‹è¯„ä¼°å·¥å…·ï¼ŒåŒ…æ‹¬ç½®ä¿¡åº¦åˆ†æã€é²æ£’æ€§æµ‹è¯•ç­‰

### æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯/æ¡†æ¶ |
|------|----------|
| åŸºç¡€æ¨¡å‹ | Qwen2.5-1.5B |
| å¾®è°ƒæ–¹æ³• | LoRA / QLoRA (4-bit) |
| è®­ç»ƒæ¡†æ¶ | HuggingFace Transformers + Trainer |
| å‚æ•°é«˜æ•ˆå¾®è°ƒ | PEFT (Parameter-Efficient Fine-Tuning) |
| é‡åŒ–åº“ | BitsAndBytes |
| æ•°æ®é›† | ChnSentiCorp (ä¸­æ–‡æƒ…æ„Ÿè¯­æ–™) |

### é¡¹ç›®ç»“æ„

```
Fine-tuning-Qwen-1.5B/
â”œâ”€â”€ configs/                      # é…ç½®æ¨¡å—
â”‚   â”œâ”€â”€ lora_config.py           # LoRA è¶…å‚æ•°é…ç½®
â”‚   â””â”€â”€ training_config.py       # è®­ç»ƒå‚æ•°é…ç½®
â”œâ”€â”€ data/                         # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ data_loader.py           # æ•°æ®é›†åŠ è½½
â”‚   â””â”€â”€ preprocessing.py         # æ•°æ®é¢„å¤„ç†
â”œâ”€â”€ model_evaluation/             # æ¨¡å‹è¯„ä¼°æ¨¡å—
â”‚   â”œâ”€â”€ run_full_eval.py         # å®Œæ•´è¯„ä¼°æµç¨‹
â”‚   â”œâ”€â”€ confidence_analysis.py   # ç½®ä¿¡åº¦åˆ†æ
â”‚   â”œâ”€â”€ robustness_test.py       # é²æ£’æ€§æµ‹è¯•
â”‚   â”œâ”€â”€ benchmark.py             # åŸºå‡†å¯¹æ¯”
â”‚   â”œâ”€â”€ report_generator.py      # è¯„ä¼°æŠ¥å‘Šç”Ÿæˆ
â”‚   â””â”€â”€ outputs/                 # è¯„ä¼°ç»“æœè¾“å‡ºï¼ˆè¯¦è§ä¸‹æ–¹è¯´æ˜ï¼‰
â”œâ”€â”€ outputs/                      # è®­ç»ƒè¾“å‡ºç›®å½•
â”‚   â””â”€â”€ lora_adapter/            # å¾®è°ƒåçš„ LoRA é€‚é…å™¨
â”œâ”€â”€ train.py                      # è®­ç»ƒä¸»è„šæœ¬
â”œâ”€â”€ eval.py                       # åŸºç¡€è¯„ä¼°è„šæœ¬
â”œâ”€â”€ inference.py                  # æ¨ç†è„šæœ¬
â”œâ”€â”€ requirements.txt              # é¡¹ç›®ä¾èµ–
â””â”€â”€ README.md                     # é¡¹ç›®æ–‡æ¡£
```

## ğŸš€ Quick Start

### ç¯å¢ƒé…ç½®

**1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰**

```bash
# # Windows
# python -m venv .venv
# .venv\Scripts\activate
å»ºè®®ç”¨conda,ä¸€äº›æ”¯æŒä¼šæ›´å¥½ã€‚
# # Linux/Mac
# python -m venv .venv
# source .venv/bin/activate
```

**2. å®‰è£…ä¾èµ–**

```bash
pip install -r requirements.txt
```

**ç¡¬ä»¶è¦æ±‚**

| æ¨¡å¼ | æœ€å°æ˜¾å­˜ | æ¨èæ˜¾å­˜ | è®­ç»ƒé€Ÿåº¦ |
|------|---------|---------|---------|
| LoRA | 8GB | 16GB | å¿« |
| QLoRA (4-bit) | 4GB | 8GB | è¾ƒæ…¢ |

### æ¨¡å‹è®­ç»ƒ

**åŸºç¡€è®­ç»ƒ**

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆLoRAï¼Œr=8ï¼‰
python train.py

# ä½¿ç”¨ QLoRA èŠ‚çœæ˜¾å­˜ï¼ˆæ¨èæ˜¾å­˜ä¸è¶³æ—¶ä½¿ç”¨ï¼‰
python train.py --use_qlora
```

**è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ**

```bash
python train.py \
    --lora_r 16 \              # LoRA ç§©ï¼ˆrankï¼‰
    --lora_alpha 32 \          # LoRA alpha å‚æ•°
    --num_epochs 5 \           # è®­ç»ƒè½®æ•°
    --batch_size 8 \           # æ‰¹æ¬¡å¤§å°
    --learning_rate 2e-4       # å­¦ä¹ ç‡
```

**æ–­ç‚¹ç»­è®­**

```bash
# è‡ªåŠ¨æ£€æµ‹å¹¶ä»æœ€æ–° checkpoint æ¢å¤
python train.py --resume_from_checkpoint auto

# ä»æŒ‡å®š checkpoint æ¢å¤
python train.py --resume_from_checkpoint ./outputs/checkpoint-500

# å¼ºåˆ¶ä»å¤´å¼€å§‹è®­ç»ƒ
python train.py --resume_from_checkpoint none
```

**è®­ç»ƒè¾“å‡º**

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹å°†ä¿å­˜åœ¨ `outputs/lora_adapter/` ç›®å½•ï¼š
- `adapter_model.safetensors` - LoRA é€‚é…å™¨æƒé‡
- `adapter_config.json` - é€‚é…å™¨é…ç½®
- `tokenizer.json` ç­‰ - åˆ†è¯å™¨æ–‡ä»¶

### æ¨¡å‹æ¨ç†

**å•æ¡æ–‡æœ¬é¢„æµ‹**

```bash
python inference.py --model_path ./outputs/lora_adapter --text "è¿™ä¸ªäº§å“è´¨é‡éå¸¸å¥½ï¼Œå€¼å¾—è´­ä¹°ï¼"
```

**äº¤äº’å¼é¢„æµ‹**

```bash
python inference.py --model_path ./outputs/lora_adapter --interactive
```

åœ¨äº¤äº’æ¨¡å¼ä¸‹ï¼Œå¯ä»¥æŒç»­è¾“å…¥æ–‡æœ¬è¿›è¡Œé¢„æµ‹ï¼Œè¾“å…¥ `quit` æˆ– `exit` é€€å‡ºã€‚

**æ‰¹é‡é¢„æµ‹**

```python
from inference import SentimentPredictor

# åˆå§‹åŒ–é¢„æµ‹å™¨
predictor = SentimentPredictor(
    base_model_name="Qwen/Qwen2.5-1.5B",
    lora_path="./outputs/lora_adapter"
)

# æ‰¹é‡é¢„æµ‹
texts = ["äº§å“å¾ˆå¥½", "è´¨é‡å¤ªå·®äº†", "ç‰©æµé€Ÿåº¦å¿«"]
results = predictor.predict_batch(texts)

for text, result in zip(texts, results):
    print(f"æ–‡æœ¬: {text}")
    print(f"é¢„æµ‹: {result['label']} (ç½®ä¿¡åº¦: {result['confidence']:.2%})")
```

## ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœè¯´æ˜

### è¯„ä¼°è¾“å‡ºæ–‡ä»¶

é¡¹ç›®åœ¨ `model_evaluation/outputs/` ç›®å½•ä¸‹ç”Ÿæˆä»¥ä¸‹è¯„ä¼°ç»“æœæ–‡ä»¶ï¼š

#### 1. **metrics.json** - åŸºç¡€æ€§èƒ½æŒ‡æ ‡

åŒ…å«æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ï¼š

```json
{
  "accuracy": 0.95,           // å‡†ç¡®ç‡
  "precision": 0.94,          // ç²¾ç¡®ç‡ï¼ˆå®å¹³å‡ï¼‰
  "recall": 0.95,             // å¬å›ç‡ï¼ˆå®å¹³å‡ï¼‰
  "f1": 0.94,                 // F1 åˆ†æ•°ï¼ˆå®å¹³å‡ï¼‰
  "auc": 0.98,                // ROC-AUC åˆ†æ•°
  "confusion_matrix": [[...], [...]]  // æ··æ·†çŸ©é˜µ
}
```

**è¯„ä»·æŒ‡æ ‡è¯´æ˜ï¼š**
- **Accuracyï¼ˆå‡†ç¡®ç‡ï¼‰**ï¼šæ­£ç¡®é¢„æµ‹çš„æ ·æœ¬å æ€»æ ·æœ¬çš„æ¯”ä¾‹
- **Precisionï¼ˆç²¾ç¡®ç‡ï¼‰**ï¼šé¢„æµ‹ä¸ºæ­£é¢çš„æ ·æœ¬ä¸­çœŸæ­£ä¸ºæ­£é¢çš„æ¯”ä¾‹
- **Recallï¼ˆå¬å›ç‡ï¼‰**ï¼šæ‰€æœ‰æ­£é¢æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
- **F1 Score**ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡å€¼
- **AUC**ï¼šROC æ›²çº¿ä¸‹é¢ç§¯ï¼Œè¡¡é‡åˆ†ç±»å™¨æ€§èƒ½

#### 2. **confidence_analysis.json** - ç½®ä¿¡åº¦åˆ†æ

åˆ†ææ¨¡å‹é¢„æµ‹çš„ç½®ä¿¡åº¦åˆ†å¸ƒå’Œå¯é æ€§ï¼š

```json
{
  "avg_confidence": 0.92,              // å¹³å‡ç½®ä¿¡åº¦
  "high_confidence_ratio": 0.85,       // é«˜ç½®ä¿¡åº¦æ ·æœ¬æ¯”ä¾‹ï¼ˆ>0.9ï¼‰
  "low_confidence_samples": [...],     // ä½ç½®ä¿¡åº¦æ ·æœ¬åˆ—è¡¨
  "confidence_distribution": {         // ç½®ä¿¡åº¦åŒºé—´åˆ†å¸ƒ
    "0.5-0.6": 50,
    "0.6-0.7": 120,
    "0.9-1.0": 1500
  }
}
```

**ç”¨é€”ï¼š**
- è¯„ä¼°æ¨¡å‹é¢„æµ‹çš„å¯é æ€§
- è¯†åˆ«æ¨¡å‹ä¸ç¡®å®šçš„æ ·æœ¬
- ä¸ºç”Ÿäº§ç¯å¢ƒè®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼

#### 3. **robustness_result.json** - é²æ£’æ€§æµ‹è¯•ç»“æœ

æµ‹è¯•æ¨¡å‹å¯¹è¾“å…¥æ‰°åŠ¨çš„æŠµæŠ—èƒ½åŠ›ï¼š

```json
{
  "original_accuracy": 0.95,
  "perturbed_accuracy": 0.89,
  "robustness_score": 0.94,           // é²æ£’æ€§å¾—åˆ†
  "perturbation_tests": {
    "synonym_replace": 0.92,          // åŒä¹‰è¯æ›¿æ¢
    "typo_insertion": 0.88,           // é”™åˆ«å­—æ’å…¥
    "punctuation_change": 0.93        // æ ‡ç‚¹å˜åŒ–
  }
}
```

**æµ‹è¯•ç±»å‹ï¼š**
- åŒä¹‰è¯æ›¿æ¢ï¼šæµ‹è¯•è¯­ä¹‰ç†è§£èƒ½åŠ›
- é”™åˆ«å­—å¹²æ‰°ï¼šæµ‹è¯•å¯¹æ‹¼å†™é”™è¯¯çš„å®¹å¿åº¦
- æ ‡ç‚¹ç¬¦å·å˜åŒ–ï¼šæµ‹è¯•å¯¹æ ¼å¼å˜åŒ–çš„é²æ£’æ€§

#### 4. **baseline_comparison.json** - åŸºçº¿æ¨¡å‹å¯¹æ¯”

å°†å¾®è°ƒæ¨¡å‹ä¸åŸºçº¿æ¨¡å‹è¿›è¡Œå¯¹æ¯”ï¼š

```json
{
  "fine_tuned_model": {
    "accuracy": 0.95,
    "f1": 0.94
  },
  "baseline_model": {
    "accuracy": 0.75,
    "f1": 0.72
  },
  "improvement": {
    "accuracy": "+20%",
    "f1": "+22%"
  }
}
```

**å¯¹æ¯”ç»´åº¦ï¼š**
- å¾®è°ƒæ¨¡å‹ vs æœªå¾®è°ƒçš„åŸºç¡€æ¨¡å‹
- å„é¡¹æŒ‡æ ‡çš„ç»å¯¹æå‡å’Œç›¸å¯¹æå‡

### è¿è¡Œå®Œæ•´è¯„ä¼°

```bash
# æ‰§è¡Œæ‰€æœ‰è¯„ä¼°æµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š
python model_evaluation/run_full_eval.py --model_path ./outputs/lora_adapter

# ä»…è¿è¡Œç‰¹å®šè¯„ä¼°
python model_evaluation/confidence_analysis.py --model_path ./outputs/lora_adapter
python model_evaluation/robustness_test.py --model_path ./outputs/lora_adapter
```

## ğŸ“ å‚æ•°è¯´æ˜

### LoRA å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--lora_r` | 8 | LoRA ç§©ï¼Œæ§åˆ¶å‚æ•°é‡ï¼ˆè¶Šå¤§æ•ˆæœè¶Šå¥½ä½†å‚æ•°è¶Šå¤šï¼‰ |
| `--lora_alpha` | 16 | LoRA ç¼©æ”¾å› å­ï¼ˆé€šå¸¸è®¾ä¸º r çš„ 2 å€ï¼‰ |
| `--lora_dropout` | 0.05 | Dropout æ¯”ä¾‹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ |

### è®­ç»ƒå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--num_epochs` | 3 | è®­ç»ƒè½®æ•° |
| `--batch_size` | 8 | æ¯æ‰¹æ¬¡æ ·æœ¬æ•° |
| `--learning_rate` | 1e-4 | å­¦ä¹ ç‡ |
| `--use_qlora` | False | æ˜¯å¦ä½¿ç”¨ QLoRA (4-bit é‡åŒ–) |

## ğŸ“š æ›´å¤šèµ„æº

- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [QLoRA è®ºæ–‡](https://arxiv.org/abs/2305.14314)
- [Qwen2.5 æ¨¡å‹æ–‡æ¡£](https://github.com/QwenLM/Qwen2.5)
- [HuggingFace PEFT åº“](https://github.com/huggingface/peft)

## ğŸ“„ License

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ã€‚

### æ¨ç†é¢„æµ‹

```bash
# äº¤äº’å¼æ¨¡å¼
python inference.py --model_path ./outputs/lora_adapter --interactive

# å•æ¡é¢„æµ‹
python inference.py --model_path ./outputs/lora_adapter --text "è¿™å®¶é¤å…çš„èœå¾ˆå¥½åƒï¼"

# è¿è¡Œç¤ºä¾‹
python inference.py --model_path ./outputs/lora_adapter --demo
```

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### LoRA åŸç†

LoRA (Low-Rank Adaptation) æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼š

```
åŸå§‹æƒé‡: W (d Ã— k)
LoRA åˆ†è§£: W + Î”W = W + BA
å…¶ä¸­: B (d Ã— r), A (r Ã— k), r << min(d, k)
```

**æ ¸å¿ƒæ€æƒ³**ï¼šå†»ç»“é¢„è®­ç»ƒæƒé‡ï¼Œåªè®­ç»ƒä½ç§©åˆ†è§£çŸ©é˜µ

```python
# LoRA é…ç½®ç¤ºä¾‹
LoraConfig(
    r=8,                    # ç§©ï¼ˆrankï¼‰
    lora_alpha=32,          # ç¼©æ”¾å› å­
    lora_dropout=0.1,       # Dropout
    target_modules=[        # ç›®æ ‡æ¨¡å—
        "q_proj", "k_proj", "v_proj", "o_proj"
    ],
)
```

### QLoRA

QLoRA åœ¨ LoRA åŸºç¡€ä¸Šå¢åŠ äº† 4-bit é‡åŒ–ï¼š

- **NF4 é‡åŒ–**ï¼šä½¿ç”¨æ­£æ€åˆ†å¸ƒæœ€ä¼˜é‡åŒ–
- **åŒé‡é‡åŒ–**ï¼šè¿›ä¸€æ­¥å‹ç¼©é‡åŒ–å¸¸æ•°
- **åˆ†é¡µä¼˜åŒ–å™¨**ï¼šå¤„ç†å†…å­˜å³°å€¼

```python
BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)
```

### è®­ç»ƒæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åŠ è½½æ¨¡å‹   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åº”ç”¨ LoRA  â”‚  â† å†»ç»“åŸå§‹æƒé‡ï¼Œæ·»åŠ ä½ç§©é€‚é…å™¨
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åŠ è½½æ•°æ®   â”‚  â† ChnSentiCorp æ•°æ®é›†
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¾®è°ƒè®­ç»ƒ   â”‚  â† åªæ›´æ–° LoRA å‚æ•° (~0.1% å‚æ•°)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä¿å­˜é€‚é…å™¨ â”‚  â† åªä¿å­˜ LoRA æƒé‡ (~10MB)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š æ•°æ®é›†

### ChnSentiCorp

ä¸­æ–‡é…’åº—è¯„è®ºæƒ…æ„Ÿåˆ†ææ•°æ®é›†ï¼š

| åˆ†å‰² | æ ·æœ¬æ•° | æ­£é¢ | è´Ÿé¢ |
|------|--------|------|------|
| è®­ç»ƒé›† | 9,600 | 4,800 | 4,800 |
| éªŒè¯é›† | 1,200 | 600 | 600 |
| æµ‹è¯•é›† | 1,200 | 600 | 600 |

**æ ·æœ¬ç¤ºä¾‹**ï¼š
```
æ­£é¢: "é…’åº—ä½ç½®å¾ˆå¥½ï¼ŒæœåŠ¡æ€åº¦ä¹Ÿéå¸¸å¥½ï¼Œæˆ¿é—´å¹²å‡€æ•´æ´ã€‚"
è´Ÿé¢: "æœåŠ¡æ€åº¦å¤ªå·®ï¼Œæˆ¿é—´ä¹Ÿå¾ˆè„ï¼Œå†ä¹Ÿä¸ä¼šæ¥äº†ã€‚"
```

## âš™ï¸ é…ç½®è¯´æ˜

### LoRA å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `r` | ç§©ï¼ˆrankï¼‰ | 4-32 |
| `lora_alpha` | ç¼©æ”¾å› å­ | r çš„ 2-4 å€ |
| `lora_dropout` | Dropout | 0.05-0.1 |
| `target_modules` | ç›®æ ‡å±‚ | q,k,v,o_proj |

### è®­ç»ƒå‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `learning_rate` | å­¦ä¹ ç‡ | 1e-4 ~ 5e-4 |
| `batch_size` | æ‰¹æ¬¡å¤§å° | 4-16 |
| `num_epochs` | è®­ç»ƒè½®æ•° | 3-5 |
| `warmup_ratio` | é¢„çƒ­æ¯”ä¾‹ | 0.1 |

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒåï¼Œåœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æœŸæ•ˆæœï¼š

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å‡†ç¡®ç‡ | ~93% |
| F1 åˆ†æ•° | ~93% |
| AUC-ROC | ~97% |

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

```bash
# ä½¿ç”¨ QLoRA
python train.py --use_qlora

# å‡å°æ‰¹æ¬¡å¤§å°
python train.py --batch_size 2

# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
python train.py --gradient_checkpointing
```

### Q2: è®­ç»ƒä¸­æ–­äº†æ€ä¹ˆåŠï¼Ÿ

è®­ç»ƒä¼šè‡ªåŠ¨ä¿å­˜ checkpointï¼ˆæ¯100æ­¥ï¼‰ï¼Œå¯ä»¥æ–­ç‚¹ç»­è®­ï¼š

```bash
# è‡ªåŠ¨æ£€æµ‹å¹¶æ¢å¤æœ€æ–°çš„ checkpoint
python train.py --resume_from_checkpoint auto

# æ‰‹åŠ¨æŒ‡å®š checkpoint
python train.py --resume_from_checkpoint ./outputs/checkpoint-500
```

**æ³¨æ„**ï¼šcheckpoint ä¿å­˜åœ¨ `outputs/checkpoint-*` ç›®å½•ï¼Œæœ€å¤šä¿ç•™3ä¸ªæœ€æ–°çš„ã€‚

### Q3: å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®ï¼Ÿ

åœ¨ `data/data_loader.py` ä¸­ä¿®æ”¹ `load_local_dataset` å‡½æ•°ï¼Œå‡†å¤‡ JSON Lines æ ¼å¼æ•°æ®ï¼š

```json
{"text": "è¯„è®ºæ–‡æœ¬", "label": 0}
{"text": "è¯„è®ºæ–‡æœ¬", "label": 1}
```

### Q4: å¦‚ä½•éƒ¨ç½²æ¨¡å‹ï¼Ÿ

```python
from inference import SentimentPredictor

predictor = SentimentPredictor(
    base_model_name="Qwen/Qwen2.5-1.5B",
    lora_path="./outputs/lora_adapter"
)

result = predictor.predict("è¿™ä¸ªäº§å“å¾ˆå¥½ç”¨ï¼")
print(result["label"])  # æ­£é¢
```

## ğŸ“– å­¦ä¹ èµ„æº

- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [QLoRA è®ºæ–‡](https://arxiv.org/abs/2305.14314)
- [PEFT æ–‡æ¡£](https://huggingface.co/docs/peft)
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [Qwen æ¨¡å‹](https://huggingface.co/Qwen)

## ğŸ“„ License

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

---

**Happy Learning! ğŸ‰**
