# ğŸ¯ ä¸­æ–‡æƒ…æ„Ÿåˆ†æå¾®è°ƒé¡¹ç›®

ä½¿ç”¨ LoRA/QLoRA æŠ€æœ¯å¾®è°ƒ Qwen-1.5B æ¨¡å‹è¿›è¡Œä¸­æ–‡æƒ…æ„Ÿåˆ†æä»»åŠ¡ã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå®Œæ•´çš„ä¸­æ–‡æƒ…æ„Ÿåˆ†æå¾®è°ƒç¤ºä¾‹ï¼Œæ—¨åœ¨å¸®åŠ©å­¦ä¹ ï¼š
- å¦‚ä½•ä½¿ç”¨ LoRA (Low-Rank Adaptation) é«˜æ•ˆå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹
- å¦‚ä½•ä½¿ç”¨ HuggingFace Trainer è¿›è¡Œæ¨¡å‹è®­ç»ƒ
- å¦‚ä½•ä½¿ç”¨ PEFT åº“è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
project/
â”œâ”€â”€ configs/                    # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ lora_config.py         # LoRA è¶…å‚æ•°é…ç½®
â”‚   â””â”€â”€ training_config.py     # è®­ç»ƒå‚æ•°é…ç½®
â”œâ”€â”€ data/                       # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py         # æ•°æ®åŠ è½½
â”‚   â””â”€â”€ preprocessing.py       # æ•°æ®é¢„å¤„ç†
â”œâ”€â”€ train.py                    # å¾®è°ƒä¸»è„šæœ¬
â”œâ”€â”€ eval.py                     # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ inference.py                # æ¨ç†ç¤ºä¾‹
â”œâ”€â”€ requirements.txt            # ä¾èµ–
â””â”€â”€ README.md                   # é¡¹ç›®è¯´æ˜
```

## ğŸ› ï¸ æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯ |
|------|------|
| åŸºç¡€æ¨¡å‹ | Qwen2.5-1.5B |
| å¾®è°ƒæŠ€æœ¯ | LoRA / QLoRA |
| è®­ç»ƒæ¡†æ¶ | HuggingFace Transformers + Trainer |
| å‚æ•°é«˜æ•ˆå¾®è°ƒ | PEFT |
| æ•°æ®é›† | ChnSentiCorp |

## ğŸ“¦ ç¯å¢ƒé…ç½®

### 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# Linux/Mac
source .venv/bin/activate
```

### 2. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 3. ç¡¬ä»¶è¦æ±‚

| é…ç½® | LoRA | QLoRA (4-bit) |
|------|------|---------------|
| æœ€å°æ˜¾å­˜ | ~8GB | ~4GB |
| æ¨èæ˜¾å­˜ | 16GB | 8GB |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### è®­ç»ƒæ¨¡å‹

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
python train.py

# ä½¿ç”¨ QLoRAï¼ˆ4-bit é‡åŒ–ï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰
python train.py --use_qlora

# è‡ªå®šä¹‰å‚æ•°
python train.py \
    --lora_r 16 \
    --lora_alpha 32 \
    --num_epochs 5 \
    --batch_size 4 \
    --learning_rate 1e-4

# æ–­ç‚¹ç»­è®­ï¼ˆè‡ªåŠ¨æ£€æµ‹æœ€æ–° checkpointï¼‰
python train.py --resume_from_checkpoint auto

# ä»æŒ‡å®š checkpoint æ¢å¤
python train.py --resume_from_checkpoint ./outputs/checkpoint-500

# å¼ºåˆ¶ä»å¤´å¼€å§‹è®­ç»ƒï¼ˆä¸ä½¿ç”¨ checkpointï¼‰
python train.py --resume_from_checkpoint none
```

### è¯„ä¼°æ¨¡å‹

```bash
python eval.py --model_path ./outputs/lora_adapter
```

### æ¨ç†é¢„æµ‹

```bash
# äº¤äº’å¼æ¨¡å¼
python inference.py --model_path ./outputs/lora_adapter --interactive

# å•æ¡é¢„æµ‹
python inference.py --model_path ./outputs/lora_adapter --text "è¿™å®¶é¤å…çš„èœå¾ˆå¥½åƒï¼"

# è¿è¡Œç¤ºä¾‹
python inference.py --model_path ./outputs/lora_adapter --demo
```

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### LoRA åŸç†

LoRA (Low-Rank Adaptation) æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼š

```
åŸå§‹æƒé‡: W (d Ã— k)
LoRA åˆ†è§£: W + Î”W = W + BA
å…¶ä¸­: B (d Ã— r), A (r Ã— k), r << min(d, k)
```

**æ ¸å¿ƒæ€æƒ³**ï¼šå†»ç»“é¢„è®­ç»ƒæƒé‡ï¼Œåªè®­ç»ƒä½ç§©åˆ†è§£çŸ©é˜µ

```python
# LoRA é…ç½®ç¤ºä¾‹
LoraConfig(
    r=8,                    # ç§©ï¼ˆrankï¼‰
    lora_alpha=32,          # ç¼©æ”¾å› å­
    lora_dropout=0.1,       # Dropout
    target_modules=[        # ç›®æ ‡æ¨¡å—
        "q_proj", "k_proj", "v_proj", "o_proj"
    ],
)
```

### QLoRA

QLoRA åœ¨ LoRA åŸºç¡€ä¸Šå¢åŠ äº† 4-bit é‡åŒ–ï¼š

- **NF4 é‡åŒ–**ï¼šä½¿ç”¨æ­£æ€åˆ†å¸ƒæœ€ä¼˜é‡åŒ–
- **åŒé‡é‡åŒ–**ï¼šè¿›ä¸€æ­¥å‹ç¼©é‡åŒ–å¸¸æ•°
- **åˆ†é¡µä¼˜åŒ–å™¨**ï¼šå¤„ç†å†…å­˜å³°å€¼

```python
BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)
```

### è®­ç»ƒæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åŠ è½½æ¨¡å‹   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åº”ç”¨ LoRA  â”‚  â† å†»ç»“åŸå§‹æƒé‡ï¼Œæ·»åŠ ä½ç§©é€‚é…å™¨
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åŠ è½½æ•°æ®   â”‚  â† ChnSentiCorp æ•°æ®é›†
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¾®è°ƒè®­ç»ƒ   â”‚  â† åªæ›´æ–° LoRA å‚æ•° (~0.1% å‚æ•°)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä¿å­˜é€‚é…å™¨ â”‚  â† åªä¿å­˜ LoRA æƒé‡ (~10MB)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š æ•°æ®é›†

### ChnSentiCorp

ä¸­æ–‡é…’åº—è¯„è®ºæƒ…æ„Ÿåˆ†ææ•°æ®é›†ï¼š

| åˆ†å‰² | æ ·æœ¬æ•° | æ­£é¢ | è´Ÿé¢ |
|------|--------|------|------|
| è®­ç»ƒé›† | 9,600 | 4,800 | 4,800 |
| éªŒè¯é›† | 1,200 | 600 | 600 |
| æµ‹è¯•é›† | 1,200 | 600 | 600 |

**æ ·æœ¬ç¤ºä¾‹**ï¼š
```
æ­£é¢: "é…’åº—ä½ç½®å¾ˆå¥½ï¼ŒæœåŠ¡æ€åº¦ä¹Ÿéå¸¸å¥½ï¼Œæˆ¿é—´å¹²å‡€æ•´æ´ã€‚"
è´Ÿé¢: "æœåŠ¡æ€åº¦å¤ªå·®ï¼Œæˆ¿é—´ä¹Ÿå¾ˆè„ï¼Œå†ä¹Ÿä¸ä¼šæ¥äº†ã€‚"
```

## âš™ï¸ é…ç½®è¯´æ˜

### LoRA å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `r` | ç§©ï¼ˆrankï¼‰ | 4-32 |
| `lora_alpha` | ç¼©æ”¾å› å­ | r çš„ 2-4 å€ |
| `lora_dropout` | Dropout | 0.05-0.1 |
| `target_modules` | ç›®æ ‡å±‚ | q,k,v,o_proj |

### è®­ç»ƒå‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `learning_rate` | å­¦ä¹ ç‡ | 1e-4 ~ 5e-4 |
| `batch_size` | æ‰¹æ¬¡å¤§å° | 4-16 |
| `num_epochs` | è®­ç»ƒè½®æ•° | 3-5 |
| `warmup_ratio` | é¢„çƒ­æ¯”ä¾‹ | 0.1 |

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒåï¼Œåœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æœŸæ•ˆæœï¼š

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å‡†ç¡®ç‡ | ~93% |
| F1 åˆ†æ•° | ~93% |
| AUC-ROC | ~97% |

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

```bash
# ä½¿ç”¨ QLoRA
python train.py --use_qlora

# å‡å°æ‰¹æ¬¡å¤§å°
python train.py --batch_size 2

# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
python train.py --gradient_checkpointing
```

### Q2: è®­ç»ƒä¸­æ–­äº†æ€ä¹ˆåŠï¼Ÿ

è®­ç»ƒä¼šè‡ªåŠ¨ä¿å­˜ checkpointï¼ˆæ¯100æ­¥ï¼‰ï¼Œå¯ä»¥æ–­ç‚¹ç»­è®­ï¼š

```bash
# è‡ªåŠ¨æ£€æµ‹å¹¶æ¢å¤æœ€æ–°çš„ checkpoint
python train.py --resume_from_checkpoint auto

# æ‰‹åŠ¨æŒ‡å®š checkpoint
python train.py --resume_from_checkpoint ./outputs/checkpoint-500
```

**æ³¨æ„**ï¼šcheckpoint ä¿å­˜åœ¨ `outputs/checkpoint-*` ç›®å½•ï¼Œæœ€å¤šä¿ç•™3ä¸ªæœ€æ–°çš„ã€‚

### Q3: å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®ï¼Ÿ

åœ¨ `data/data_loader.py` ä¸­ä¿®æ”¹ `load_local_dataset` å‡½æ•°ï¼Œå‡†å¤‡ JSON Lines æ ¼å¼æ•°æ®ï¼š

```json
{"text": "è¯„è®ºæ–‡æœ¬", "label": 0}
{"text": "è¯„è®ºæ–‡æœ¬", "label": 1}
```

### Q4: å¦‚ä½•éƒ¨ç½²æ¨¡å‹ï¼Ÿ

```python
from inference import SentimentPredictor

predictor = SentimentPredictor(
    base_model_name="Qwen/Qwen2.5-1.5B",
    lora_path="./outputs/lora_adapter"
)

result = predictor.predict("è¿™ä¸ªäº§å“å¾ˆå¥½ç”¨ï¼")
print(result["label"])  # æ­£é¢
```

## ğŸ“– å­¦ä¹ èµ„æº

- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [QLoRA è®ºæ–‡](https://arxiv.org/abs/2305.14314)
- [PEFT æ–‡æ¡£](https://huggingface.co/docs/peft)
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [Qwen æ¨¡å‹](https://huggingface.co/Qwen)

## ğŸ“„ License

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

---

**Happy Learning! ğŸ‰**
