# ğŸ¯ Qwen-1.5B ä¸­æ–‡æƒ…æ„Ÿåˆ†æå¾®è°ƒé¡¹ç›®

> åŸºäº LoRA/QLoRA æŠ€æœ¯å¾®è°ƒ Qwen2.5-1.5B æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆçš„ä¸­æ–‡æƒ…æ„Ÿåˆ†æä»»åŠ¡

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—_Transformers-4.36+-yellow.svg)](https://huggingface.co/docs/transformers)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸŒŸ é¡¹ç›®äº®ç‚¹

- ğŸ† **é«˜æ€§èƒ½**ï¼šLabel Scoring æ–¹æ¡ˆè¾¾åˆ° **95.75%** å‡†ç¡®ç‡
- ğŸ’¡ **åŒæ–¹æ¡ˆå¯¹æ¯”**ï¼šåˆ¤åˆ«å¼åˆ†ç±»å¤´ vs ç”Ÿæˆå¼ Label Scoring å…¨é¢å¯¹æ¯”
- âš¡ **é«˜æ•ˆè®­ç»ƒ**ï¼šLoRA/QLoRA æŠ€æœ¯ï¼Œä»…éœ€å¾®è°ƒ ~1% å‚æ•°
- ğŸ’¾ **ä½æ˜¾å­˜å ç”¨**ï¼šQLoRA æ¨¡å¼ä¸‹ä»…éœ€ 4GB æ˜¾å­˜å³å¯è®­ç»ƒ
- ğŸ“Š **å®Œæ•´è¯„ä¼°**ï¼šç½®ä¿¡åº¦åˆ†æã€é²æ£’æ€§æµ‹è¯•ã€å¤šæ–¹æ¡ˆåŸºå‡†å¯¹æ¯”
- ğŸ”§ **ç”Ÿäº§å°±ç»ª**ï¼šå®Œæ•´çš„è®­ç»ƒã€è¯„ä¼°ã€æ¨ç†æµç¨‹

## ğŸ“‹ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç«¯åˆ°ç«¯ä¸­æ–‡æƒ…æ„Ÿåˆ†æè§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ã€‚é¡¹ç›®ç‰¹ç‚¹ï¼š

- **é«˜æ•ˆå¾®è°ƒ**ï¼šä½¿ç”¨ LoRA/QLoRA æŠ€æœ¯ï¼Œä»…éœ€å¾®è°ƒå°‘é‡å‚æ•°ï¼ˆçº¦ 1% æ¨¡å‹å‚æ•°ï¼‰
- **ä½æ˜¾å­˜å ç”¨**ï¼šQLoRA æ¨¡å¼ä¸‹ä»…éœ€ 4GB æ˜¾å­˜å³å¯è®­ç»ƒ
- **å®Œæ•´å·¥ä½œæµ**ï¼šæ¶µç›–æ•°æ®å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€æ¨ç†å…¨æµç¨‹
- **è¯¦ç»†è¯„ä¼°**ï¼šæä¾›å¤šç»´åº¦æ¨¡å‹è¯„ä¼°å·¥å…·ï¼ŒåŒ…æ‹¬ç½®ä¿¡åº¦åˆ†æã€é²æ£’æ€§æµ‹è¯•ç­‰

### æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯/æ¡†æ¶ |
|------|----------|
| åŸºç¡€æ¨¡å‹ | Qwen2.5-1.5B |
| å¾®è°ƒæ–¹æ³• | LoRA / QLoRA (4-bit) |
| è®­ç»ƒæ¡†æ¶ | HuggingFace Transformers + Trainer |
| å‚æ•°é«˜æ•ˆå¾®è°ƒ | PEFT (Parameter-Efficient Fine-Tuning) |
| é‡åŒ–åº“ | BitsAndBytes |
| æ•°æ®é›† | ChnSentiCorp (ä¸­æ–‡æƒ…æ„Ÿè¯­æ–™) |

### é¡¹ç›®ç»“æ„

```
Fine-tuning-Qwen-1.5B/
â”œâ”€â”€ README.md                     # é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ pyproject.toml               # é¡¹ç›®é…ç½®
â”œâ”€â”€ requirements.txt             # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ requirements_autodl.txt      # AutoDL ç¯å¢ƒä¾èµ–
â”‚
â”œâ”€â”€ configs/                     # é…ç½®æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ lora_config.py          # LoRA è¶…å‚æ•°é…ç½®
â”‚   â””â”€â”€ training_config.py      # è®­ç»ƒå‚æ•°é…ç½®
â”‚
â”œâ”€â”€ data/                        # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py          # æ•°æ®é›†åŠ è½½
â”‚   â””â”€â”€ preprocessing.py        # æ•°æ®é¢„å¤„ç†
â”‚
â”œâ”€â”€ scripts/                     # è®­ç»ƒå’Œæ¨ç†è„šæœ¬
â”‚   â”œâ”€â”€ train_classifier.py     # æ–¹æ¡ˆä¸€ï¼šLoRA + åˆ†ç±»å¤´
â”‚   â”œâ”€â”€ train_label_scoring.py  # æ–¹æ¡ˆäºŒï¼šLoRA + Label Scoring
â”‚   â”œâ”€â”€ evaluate.py             # åŸºç¡€è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ inference.py            # æ¨ç†è„šæœ¬
â”‚
â”œâ”€â”€ evaluation/                  # æ¨¡å‹è¯„ä¼°æ¨¡å—
â”‚   â”œâ”€â”€ run_full_eval.py        # å®Œæ•´è¯„ä¼°æµç¨‹
â”‚   â”œâ”€â”€ confidence_analysis.py  # ç½®ä¿¡åº¦åˆ†æ
â”‚   â”œâ”€â”€ robustness_test.py      # é²æ£’æ€§æµ‹è¯•
â”‚   â”œâ”€â”€ benchmark.py            # åŸºå‡†å¯¹æ¯”
â”‚   â”œâ”€â”€ zero_shot_baseline.py  # Zero-shot åŸºçº¿
â”‚   â”œâ”€â”€ report_generator.py     # è¯„ä¼°æŠ¥å‘Šç”Ÿæˆ
â”‚   â””â”€â”€ outputs/                # è¯„ä¼°ç»“æœè¾“å‡º
â”‚       â”œâ”€â”€ baseline_comparison.json
â”‚       â”œâ”€â”€ confidence_analysis.json
â”‚       â”œâ”€â”€ metrics.json
â”‚       â””â”€â”€ robustness_result.json
â”‚
â””â”€â”€ experiments/                 # å®éªŒç»“æœç›®å½•
    â”œâ”€â”€ classifier_head/        # æ–¹æ¡ˆä¸€è®­ç»ƒè¾“å‡º
    â”‚   â”œâ”€â”€ lora_adapter/      # LoRA é€‚é…å™¨
    â”‚   â”œâ”€â”€ train_results.json
    â”‚   â””â”€â”€ test_results.json
    â””â”€â”€ label_scoring/          # æ–¹æ¡ˆäºŒè®­ç»ƒè¾“å‡º
        â”œâ”€â”€ lora_adapter/      # LoRA é€‚é…å™¨
        â”œâ”€â”€ train_results.json
        â””â”€â”€ test_results.json
```

## ğŸš€ Quick Start

### ä¸¤ç§å¾®è°ƒæ–¹æ¡ˆå¯¹æ¯”

æœ¬é¡¹ç›®å®ç°äº†ä¸¤ç§ä¸åŒçš„å¾®è°ƒæ–¹æ¡ˆï¼Œé€‚ç”¨äºä¸åŒçš„åº”ç”¨åœºæ™¯ï¼š

#### **æ–¹æ¡ˆä¸€ï¼šLoRA + åˆ¤åˆ«å¼åˆ†ç±»å¤´**ï¼ˆ`train_classifier.py`ï¼‰

ä½¿ç”¨ `AutoModelForSequenceClassification`ï¼Œåœ¨æ¨¡å‹é¡¶éƒ¨æ·»åŠ çº¿æ€§åˆ†ç±»å±‚ã€‚

```bash
python scripts/train_classifier.py --lora_r 8 --num_epochs 3
```

**ç‰¹ç‚¹ï¼š**
- ğŸ¯ ä½¿ç”¨ä¼ ç»Ÿåˆ†ç±»å¤´ï¼ˆLinear layer: hidden_dim â†’ 2ï¼‰
- ğŸ“Š è¾“å‡ºäºŒç»´ logitsï¼Œé€šè¿‡ softmax å¾—åˆ°æ¦‚ç‡
- âš¡ è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦å¿«
- ğŸ“ é€‚åˆæ ‡å‡†åˆ†ç±»ä»»åŠ¡

**æ€§èƒ½æŒ‡æ ‡ï¼š**
| æŒ‡æ ‡ | æ•°å€¼ |
|---|---|
| å‡†ç¡®ç‡ | 93.8% |
| F1 åˆ†æ•° | 93.7% |
| ç²¾ç¡®ç‡ | 97.1% |
| å¬å›ç‡ | 90.6% |

#### **æ–¹æ¡ˆäºŒï¼šLoRA + ç”Ÿæˆå¼ Label Scoring**ï¼ˆ`train_label_scoring.py`ï¼‰

ä½¿ç”¨ `AutoModelForCausalLM`ï¼Œé€šè¿‡æ¯”è¾ƒå€™é€‰æ ‡ç­¾çš„ç”Ÿæˆæ¦‚ç‡è¿›è¡Œåˆ†ç±»ã€‚

```bash
python scripts/train_label_scoring.py --lora_r 8 --num_epochs 3
```

**ç‰¹ç‚¹ï¼š**
- ğŸ”® å¤ç”¨è¯­è¨€æ¨¡å‹çš„ LM Headï¼ˆæ— é¢å¤–åˆ†ç±»å±‚ï¼‰
- ğŸ“ Prompt æ¨¡æ¿ï¼š`"è¯„è®ºï¼š{text}\næƒ…æ„Ÿå€¾å‘ï¼š"`
- ğŸ² æ¯”è¾ƒ "æ­£é¢" å’Œ "è´Ÿé¢" çš„ token log-probability
- ğŸŒŸ æ›´è‡ªç„¶çš„è¯­ä¹‰å¯¹é½ï¼Œé€‚åˆå°‘æ ·æœ¬å’Œè·¨é¢†åŸŸåœºæ™¯

**æ€§èƒ½æŒ‡æ ‡ï¼š**
| æŒ‡æ ‡ | æ•°å€¼ |
|---|---|
| **å‡†ç¡®ç‡** | **95.75%** â­ |
| **F1 åˆ†æ•°** | **95.80%** â­ |
| **ç²¾ç¡®ç‡** | **95.88%** |
| **å¬å›ç‡** | **95.72%** |

#### **Zero-shot åŸºçº¿**ï¼ˆ`zero_shot_baseline.py`ï¼‰

ä¸è¿›è¡Œä»»ä½•å¾®è°ƒï¼Œç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ + prompt è¿›è¡Œé¢„æµ‹ã€‚

```bash
python evaluation/zero_shot_baseline.py
```

**æ€§èƒ½æŒ‡æ ‡ï¼š**
| æŒ‡æ ‡ | æ•°å€¼ |
|---|---|
| å‡†ç¡®ç‡ | 88.0% |
| F1 åˆ†æ•° | 88.2% |

---

**ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”æ€»ç»“ï¼š**

| æ–¹æ¡ˆ | å‡†ç¡®ç‡ | F1 | ç›¸å¯¹æå‡ | æ¨èåœºæ™¯ |
|---|---|---|---|---|
| **Label Scoring** | **95.75%** | **95.80%** | +1.95% | å°‘æ ·æœ¬ã€è·¨é¢†åŸŸã€è¯­ä¹‰å¯¹é½è¦æ±‚é«˜ |
| **Classifier Head** | 93.8% | 93.7% | åŸºå‡† | æ ‡å‡†åˆ†ç±»ä»»åŠ¡ã€è¿½æ±‚æ¨ç†é€Ÿåº¦ |
| **Zero-shot** | 88.0% | 88.2% | -5.8% | æ— æ ‡æ³¨æ•°æ®ã€å¿«é€ŸéªŒè¯ |

> ğŸ’¡ **é€‰æ‹©å»ºè®®**ï¼šLabel Scoring æ–¹æ¡ˆåœ¨æœ¬é¡¹ç›®ä¸­è¡¨ç°æœ€ä½³ï¼ˆ+1.95%ï¼‰ï¼Œä¸”æ— éœ€é¢å¤–åˆ†ç±»å±‚å‚æ•°ï¼Œæ¨èä½œä¸ºé»˜è®¤æ–¹æ¡ˆã€‚

### ç¯å¢ƒé…ç½®

**1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰**

```bash
# # Windows
# python -m venv .venv
# .venv\Scripts\activate
å»ºè®®ç”¨conda,ä¸€äº›æ”¯æŒä¼šæ›´å¥½ã€‚
# # Linux/Mac
# python -m venv .venv
# source .venv/bin/activate
```

**2. å®‰è£…ä¾èµ–**

```bash
pip install -r requirements.txt
```

> **Windows ç”¨æˆ·æ³¨æ„**ï¼šé¡¹ç›®å·²é…ç½®è‡ªåŠ¨å®‰è£… `bitsandbytes-windows` ä»¥è§£å†³ Windows å…¼å®¹æ€§é—®é¢˜ã€‚å¦‚é‡åˆ° bitsandbytes ç›¸å…³é”™è¯¯ï¼Œè¯·æ‰‹åŠ¨æ‰§è¡Œï¼š
> ```bash
> pip uninstall bitsandbytes -y
> pip install bitsandbytes-windows
> ```

**ç¡¬ä»¶è¦æ±‚**

| æ¨¡å¼ | æœ€å°æ˜¾å­˜ | æ¨èæ˜¾å­˜ | è®­ç»ƒé€Ÿåº¦ |
|------|---------|---------|---------|
| LoRA | 8GB | 16GB | å¿« |
| QLoRA (4-bit) | 4GB | 8GB | è¾ƒæ…¢ |

### æ¨¡å‹è®­ç»ƒ

**åŸºç¡€è®­ç»ƒï¼ˆæ–¹æ¡ˆä¸€ï¼šåˆ†ç±»å¤´ï¼‰**

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆLoRAï¼Œr=8ï¼‰
python scripts/train_classifier.py

# ä½¿ç”¨ QLoRA èŠ‚çœæ˜¾å­˜ï¼ˆæ¨èæ˜¾å­˜ä¸è¶³æ—¶ä½¿ç”¨ï¼‰
python scripts/train_classifier.py --use_qlora
```

**Label Scoring è®­ç»ƒï¼ˆæ–¹æ¡ˆäºŒï¼šæ¨èï¼‰**

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®
python scripts/train_label_scoring.py

# ä½¿ç”¨ QLoRA
python scripts/train_label_scoring.py --use_qlora
```

**è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ**

```bash
python scripts/train_classifier.py \
    --lora_r 16 \              # LoRA ç§©ï¼ˆrankï¼‰
    --lora_alpha 32 \          # LoRA alpha å‚æ•°
    --num_epochs 5 \           # è®­ç»ƒè½®æ•°
    --batch_size 8 \           # æ‰¹æ¬¡å¤§å°
    --learning_rate 2e-4       # å­¦ä¹ ç‡
```

**æ–­ç‚¹ç»­è®­**

```bash
# è‡ªåŠ¨æ£€æµ‹å¹¶ä»æœ€æ–° checkpoint æ¢å¤
python scripts/train_classifier.py --resume_from_checkpoint auto

# ä»æŒ‡å®š checkpoint æ¢å¤
python scripts/train_classifier.py --resume_from_checkpoint ./experiments/classifier_head/checkpoint-500

# å¼ºåˆ¶ä»å¤´å¼€å§‹è®­ç»ƒ
python scripts/train_classifier.py --resume_from_checkpoint none
```

**è®­ç»ƒè¾“å‡º**

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹å°†ä¿å­˜åœ¨ç›¸åº”çš„å®éªŒç›®å½•ï¼š
- **æ–¹æ¡ˆä¸€**ï¼š`experiments/classifier_head/lora_adapter/`
- **æ–¹æ¡ˆäºŒ**ï¼š`experiments/label_scoring/lora_adapter/`

è¾“å‡ºæ–‡ä»¶åŒ…æ‹¬ï¼š
- `adapter_model.safetensors` - LoRA é€‚é…å™¨æƒé‡
- `adapter_config.json` - é€‚é…å™¨é…ç½®
- `tokenizer.json` ç­‰ - åˆ†è¯å™¨æ–‡ä»¶
- `label_scoring_meta.json` - Label Scoring æ–¹æ¡ˆçš„å…ƒä¿¡æ¯ï¼ˆä»…æ–¹æ¡ˆäºŒï¼‰

### æ¨¡å‹æ¨ç†

**å•æ¡æ–‡æœ¬é¢„æµ‹**

```bash
# æ–¹æ¡ˆä¸€ï¼šåˆ†ç±»å¤´æ¨¡å‹
python scripts/inference.py \
    --model_path ./experiments/classifier_head/lora_adapter \
    --text "è¿™ä¸ªäº§å“è´¨é‡éå¸¸å¥½ï¼Œå€¼å¾—è´­ä¹°ï¼"

# æ–¹æ¡ˆäºŒï¼šLabel Scoring æ¨¡å‹
python scripts/inference.py \
    --model_path ./experiments/label_scoring/lora_adapter \
    --text "è¿™ä¸ªäº§å“è´¨é‡éå¸¸å¥½ï¼Œå€¼å¾—è´­ä¹°ï¼"
```

**äº¤äº’å¼é¢„æµ‹**

```bash
python scripts/inference.py \
    --model_path ./experiments/classifier_head/lora_adapter \
    --interactive
```

åœ¨äº¤äº’æ¨¡å¼ä¸‹ï¼Œå¯ä»¥æŒç»­è¾“å…¥æ–‡æœ¬è¿›è¡Œé¢„æµ‹ï¼Œè¾“å…¥ `quit` æˆ– `exit` é€€å‡ºã€‚

**æ‰¹é‡é¢„æµ‹**

```python
from scripts.inference import SentimentPredictor

# åˆå§‹åŒ–é¢„æµ‹å™¨
predictor = SentimentPredictor(
    base_model_name="Qwen/Qwen2.5-1.5B",
    lora_path="./experiments/classifier_head/lora_adapter"
)

# æ‰¹é‡é¢„æµ‹
texts = ["äº§å“å¾ˆå¥½", "è´¨é‡å¤ªå·®äº†", "ç‰©æµé€Ÿåº¦å¿«"]
results = predictor.predict_batch(texts)

for text, result in zip(texts, results):
    print(f"æ–‡æœ¬: {text}")
    print(f"é¢„æµ‹: {result['label']} (ç½®ä¿¡åº¦: {result['confidence']:.2%})")
```

## ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœè¯´æ˜

### è¯„ä¼°è¾“å‡ºæ–‡ä»¶

é¡¹ç›®åœ¨ `evaluation/outputs/` ç›®å½•ä¸‹ç”Ÿæˆä»¥ä¸‹è¯„ä¼°ç»“æœæ–‡ä»¶ï¼š

#### 1. **baseline_comparison.json** - å¤šæ–¹æ¡ˆå¯¹æ¯”

å®Œæ•´å¯¹æ¯”ä»¥ä¸‹ä¸‰ç§æ–¹æ¡ˆï¼ˆè¿˜æœ‰ä¸€ç§Qwen-1.5B å†»ç»“ + LoRAåˆ†ç±»å¤´åˆ—åœ¨evaluation\outputs\baseline_comparison.jsoné‡Œé¢ï¼Œä¸å¤ªå…·æœ‰å‚è€ƒä»·å€¼ï¼Œåªèƒ½è¯´æ˜loraè®­ç»ƒçš„hidden stateå’Œåˆ†ç±»å¤´å¼ºç»‘å®šï¼‰çš„æ€§èƒ½è¡¨ç°ï¼š

```json
{
  "qwen_lora_label_scoring": {
    "name": "Qwen-1.5B + LoRAå¾®è°ƒ (Label Scoring)",
    "description": "ä½¿ç”¨ CausalLM + LoRA å¾®è°ƒï¼Œç”Ÿæˆå¼åˆ†ç±»æ¥å£",
    "metrics": {
      "accuracy": 0.9575,    // å‡†ç¡®ç‡ 95.75%
      "precision": 0.9588,   // ç²¾ç¡®ç‡
      "recall": 0.9572,      // å¬å›ç‡
      "f1": 0.9580          // F1 åˆ†æ•°
    }
  },
  "qwen_lora": {
    "name": "Qwen-1.5B + LoRAå¾®è°ƒ",
    "description": "LoRAå¾®è°ƒçš„å®Œæ•´æ¨¡å‹ï¼ˆåŸºç¡€æ¨¡å‹+åˆ†ç±»å¤´ï¼‰",
    "metrics": {
      "accuracy": 0.938,     // å‡†ç¡®ç‡ 93.8%
      "f1": 0.937           // F1 åˆ†æ•°
    }
  },
  "qwen_zero_shot": {
    "name": "Qwen-1.5B Zero-shot",
    "description": "ä¸å¾®è°ƒï¼Œä½¿ç”¨ prompt æ¨¡æ¿ç›´æ¥é¢„æµ‹",
    "metrics": {
      "accuracy": 0.88,      // å‡†ç¡®ç‡ 88.0%
      "f1": 0.882           // F1 åˆ†æ•°
    }
  }
}
```

**å¯¹æ¯”æ´å¯Ÿï¼š**
- âœ¨ **Label Scoring æ–¹æ¡ˆ** å–å¾—æœ€ä½³æ€§èƒ½ï¼ˆ95.75%ï¼‰ï¼Œæ¯”ä¼ ç»Ÿåˆ†ç±»å¤´é«˜ 1.95%
- ğŸ“ˆ ç›¸æ¯” Zero-shot åŸºçº¿æå‡ 7.75 ä¸ªç™¾åˆ†ç‚¹
- ğŸš€ ä¸¤ç§å¾®è°ƒæ–¹æ¡ˆéƒ½æ˜¾è‘—è¶…è¶Šæœªå¾®è°ƒæ¨¡å‹


### ä»¥ä¸‹æ€§èƒ½æŒ‡æ ‡å‡åŸºäºæ–¹æ¡ˆä¸€ï¼šåˆ†ç±»å¤´æ¨¡å‹ï¼Œæ–¹æ¡ˆäºŒä¸‹é¢æµ‹è¯•çš„è¯„ä¼°ä»£ç è¿˜æ²¡æ¥å¾—åŠæ”¹ï¼Œæš‚æ—¶æŒ–ä¸ªå‘ã€‚
#### 2. **metrics.json** - åŸºç¡€æ€§èƒ½æŒ‡æ ‡

åŒ…å«æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ï¼š

```json
{
  "accuracy": 0.95,           // å‡†ç¡®ç‡
  "precision": 0.94,          // ç²¾ç¡®ç‡ï¼ˆå®å¹³å‡ï¼‰
  "recall": 0.95,             // å¬å›ç‡ï¼ˆå®å¹³å‡ï¼‰
  "f1": 0.94,                 // F1 åˆ†æ•°ï¼ˆå®å¹³å‡ï¼‰
  "auc": 0.98,                // ROC-AUC åˆ†æ•°
  "confusion_matrix": [[...], [...]]  // æ··æ·†çŸ©é˜µ
}
```

**è¯„ä»·æŒ‡æ ‡è¯´æ˜ï¼š**
- **Accuracyï¼ˆå‡†ç¡®ç‡ï¼‰**ï¼šæ­£ç¡®é¢„æµ‹çš„æ ·æœ¬å æ€»æ ·æœ¬çš„æ¯”ä¾‹
- **Precisionï¼ˆç²¾ç¡®ç‡ï¼‰**ï¼šé¢„æµ‹ä¸ºæ­£é¢çš„æ ·æœ¬ä¸­çœŸæ­£ä¸ºæ­£é¢çš„æ¯”ä¾‹
- **Recallï¼ˆå¬å›ç‡ï¼‰**ï¼šæ‰€æœ‰æ­£é¢æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
- **F1 Score**ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡å€¼
- **AUC**ï¼šROC æ›²çº¿ä¸‹é¢ç§¯ï¼Œè¡¡é‡åˆ†ç±»å™¨æ€§èƒ½

#### 3. **confidence_analysis.json** - ç½®ä¿¡åº¦åˆ†æ

åˆ†ææ¨¡å‹é¢„æµ‹çš„ç½®ä¿¡åº¦åˆ†å¸ƒå’Œå¯é æ€§ï¼š

```json
{
  "avg_confidence": 0.92,              // å¹³å‡ç½®ä¿¡åº¦
  "high_confidence_ratio": 0.85,       // é«˜ç½®ä¿¡åº¦æ ·æœ¬æ¯”ä¾‹ï¼ˆ>0.9ï¼‰
  "low_confidence_samples": [...],     // ä½ç½®ä¿¡åº¦æ ·æœ¬åˆ—è¡¨
  "confidence_distribution": {         // ç½®ä¿¡åº¦åŒºé—´åˆ†å¸ƒ
    "0.5-0.6": 50,
    "0.6-0.7": 120,
    "0.9-1.0": 1500
  }
}
```

**ç”¨é€”ï¼š**
- è¯„ä¼°æ¨¡å‹é¢„æµ‹çš„å¯é æ€§
- è¯†åˆ«æ¨¡å‹ä¸ç¡®å®šçš„æ ·æœ¬
- ä¸ºç”Ÿäº§ç¯å¢ƒè®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼

#### 4. **robustness_result.json** - é²æ£’æ€§æµ‹è¯•ç»“æœ

æµ‹è¯•æ¨¡å‹å¯¹è¾“å…¥æ‰°åŠ¨çš„æŠµæŠ—èƒ½åŠ›ï¼š

```json
{
  "original_accuracy": 0.95,
  "perturbed_accuracy": 0.89,
  "robustness_score": 0.94,           // é²æ£’æ€§å¾—åˆ†
  "perturbation_tests": {
    "synonym_replace": 0.92,          // åŒä¹‰è¯æ›¿æ¢
    "typo_insertion": 0.88,           // é”™åˆ«å­—æ’å…¥
    "punctuation_change": 0.93        // æ ‡ç‚¹å˜åŒ–
  }
}
```

**æµ‹è¯•ç±»å‹ï¼š**
- åŒä¹‰è¯æ›¿æ¢ï¼šæµ‹è¯•è¯­ä¹‰ç†è§£èƒ½åŠ›
- é”™åˆ«å­—å¹²æ‰°ï¼šæµ‹è¯•å¯¹æ‹¼å†™é”™è¯¯çš„å®¹å¿åº¦
- æ ‡ç‚¹ç¬¦å·å˜åŒ–ï¼šæµ‹è¯•å¯¹æ ¼å¼å˜åŒ–çš„é²æ£’æ€§

### è¿è¡Œå®Œæ•´è¯„ä¼°

```bash
# æ‰§è¡Œæ‰€æœ‰è¯„ä¼°æµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Šï¼ˆæ–¹æ¡ˆä¸€ï¼‰
python evaluation/run_full_eval.py \
    --model_path ./experiments/classifier_head/lora_adapter

# æ‰§è¡Œæ‰€æœ‰è¯„ä¼°æµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Šï¼ˆæ–¹æ¡ˆäºŒï¼‰
python evaluation/run_full_eval.py \
    --model_path ./experiments/label_scoring/lora_adapter

# ä»…è¿è¡Œç‰¹å®šè¯„ä¼°
python evaluation/confidence_analysis.py \
    --model_path ./experiments/classifier_head/lora_adapter
    
python evaluation/robustness_test.py \
    --model_path ./experiments/classifier_head/lora_adapter
```

## ğŸ“ å‚æ•°è¯´æ˜

### LoRA å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--lora_r` | 8 | LoRA ç§©ï¼Œæ§åˆ¶å‚æ•°é‡ï¼ˆè¶Šå¤§æ•ˆæœè¶Šå¥½ä½†å‚æ•°è¶Šå¤šï¼‰ |
| `--lora_alpha` | 16 | LoRA ç¼©æ”¾å› å­ï¼ˆé€šå¸¸è®¾ä¸º r çš„ 2 å€ï¼‰ |
| `--lora_dropout` | 0.05 | Dropout æ¯”ä¾‹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ |

### è®­ç»ƒå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--num_epochs` | 3 | è®­ç»ƒè½®æ•° |
| `--batch_size` | 8 | æ¯æ‰¹æ¬¡æ ·æœ¬æ•° |
| `--learning_rate` | 1e-4 | å­¦ä¹ ç‡ |
| `--use_qlora` | False | æ˜¯å¦ä½¿ç”¨ QLoRA (4-bit é‡åŒ–) |

## ğŸ“š æ›´å¤šèµ„æº

- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [QLoRA è®ºæ–‡](https://arxiv.org/abs/2305.14314)
- [Qwen2.5 æ¨¡å‹æ–‡æ¡£](https://github.com/QwenLM/Qwen2.5)
- [HuggingFace PEFT åº“](https://github.com/huggingface/peft)

## ğŸ“„ License

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ã€‚

### æ¨ç†é¢„æµ‹

```bash
# äº¤äº’å¼æ¨¡å¼ï¼ˆæ–¹æ¡ˆä¸€ï¼‰
python scripts/inference.py \
    --model_path ./experiments/classifier_head/lora_adapter \
    --interactive

# å•æ¡é¢„æµ‹ï¼ˆæ–¹æ¡ˆäºŒï¼‰
python scripts/inference.py \
    --model_path ./experiments/label_scoring/lora_adapter \
    --text "è¿™å®¶é¤å…çš„èœå¾ˆå¥½åƒï¼"

# è¿è¡Œç¤ºä¾‹
python scripts/inference.py \
    --model_path ./experiments/classifier_head/lora_adapter \
    --demo
```

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### LoRA åŸç†

LoRA (Low-Rank Adaptation) æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼š

```
åŸå§‹æƒé‡: W (d Ã— k)
LoRA åˆ†è§£: W + Î”W = W + BA
å…¶ä¸­: B (d Ã— r), A (r Ã— k), r << min(d, k)
```

**æ ¸å¿ƒæ€æƒ³**ï¼šå†»ç»“é¢„è®­ç»ƒæƒé‡ï¼Œåªè®­ç»ƒä½ç§©åˆ†è§£çŸ©é˜µ

```python
# LoRA é…ç½®ç¤ºä¾‹
LoraConfig(
    r=8,                    # ç§©ï¼ˆrankï¼‰
    lora_alpha=32,          # ç¼©æ”¾å› å­
    lora_dropout=0.1,       # Dropout
    target_modules=[        # ç›®æ ‡æ¨¡å—
        "q_proj", "k_proj", "v_proj", "o_proj"
    ],
)
```

### QLoRA

QLoRA åœ¨ LoRA åŸºç¡€ä¸Šå¢åŠ äº† 4-bit é‡åŒ–ï¼š

- **NF4 é‡åŒ–**ï¼šä½¿ç”¨æ­£æ€åˆ†å¸ƒæœ€ä¼˜é‡åŒ–
- **åŒé‡é‡åŒ–**ï¼šè¿›ä¸€æ­¥å‹ç¼©é‡åŒ–å¸¸æ•°
- **åˆ†é¡µä¼˜åŒ–å™¨**ï¼šå¤„ç†å†…å­˜å³°å€¼

```python
BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)
```

### è®­ç»ƒæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åŠ è½½æ¨¡å‹   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åº”ç”¨ LoRA  â”‚  â† å†»ç»“åŸå§‹æƒé‡ï¼Œæ·»åŠ ä½ç§©é€‚é…å™¨
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åŠ è½½æ•°æ®   â”‚  â† ChnSentiCorp æ•°æ®é›†
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¾®è°ƒè®­ç»ƒ   â”‚  â† åªæ›´æ–° LoRA å‚æ•° (~0.1% å‚æ•°)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä¿å­˜é€‚é…å™¨ â”‚  â† åªä¿å­˜ LoRA æƒé‡ (~10MB)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š æ•°æ®é›†

### ChnSentiCorp

ä¸­æ–‡é…’åº—è¯„è®ºæƒ…æ„Ÿåˆ†ææ•°æ®é›†ï¼š

| åˆ†å‰² | æ ·æœ¬æ•° | æ­£é¢ | è´Ÿé¢ |
|------|--------|------|------|
| è®­ç»ƒé›† | 9,600 | 4,800 | 4,800 |
| éªŒè¯é›† | 1,200 | 600 | 600 |
| æµ‹è¯•é›† | 1,200 | 600 | 600 |

**æ ·æœ¬ç¤ºä¾‹**ï¼š
```
æ­£é¢: "é…’åº—ä½ç½®å¾ˆå¥½ï¼ŒæœåŠ¡æ€åº¦ä¹Ÿéå¸¸å¥½ï¼Œæˆ¿é—´å¹²å‡€æ•´æ´ã€‚"
è´Ÿé¢: "æœåŠ¡æ€åº¦å¤ªå·®ï¼Œæˆ¿é—´ä¹Ÿå¾ˆè„ï¼Œå†ä¹Ÿä¸ä¼šæ¥äº†ã€‚"
```

## âš™ï¸ é…ç½®è¯´æ˜

### LoRA å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `r` | ç§©ï¼ˆrankï¼‰ | 4-32 |
| `lora_alpha` | ç¼©æ”¾å› å­ | r çš„ 2-4 å€ |
| `lora_dropout` | Dropout | 0.05-0.1 |
| `target_modules` | ç›®æ ‡å±‚ | q,k,v,o_proj |

### è®­ç»ƒå‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `learning_rate` | å­¦ä¹ ç‡ | 1e-4 ~ 5e-4 |
| `batch_size` | æ‰¹æ¬¡å¤§å° | 4-16 |
| `num_epochs` | è®­ç»ƒè½®æ•° | 3-5 |
| `warmup_ratio` | é¢„çƒ­æ¯”ä¾‹ | 0.1 |

## ğŸ“ˆ å®éªŒç»“æœ

ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒåï¼Œåœ¨æµ‹è¯•é›†ä¸Šçš„å®é™…æ•ˆæœï¼š

### æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | å‡†ç¡®ç‡ | ç²¾ç¡®ç‡ | å¬å›ç‡ | F1 åˆ†æ•° | å¤‡æ³¨ |
|------|--------|--------|--------|---------|------|
| **LoRA + Label Scoring** | **95.75%** | **95.88%** | **95.72%** | **95.80%** | ğŸ† æœ€ä½³æ–¹æ¡ˆ |
| LoRA + Classifier Head | 93.8% | 97.1% | 90.6% | 93.7% | æ ‡å‡†æ–¹æ¡ˆ |
| Zero-shot Baseline | 88.0% | 89.6% | 86.9% | 88.2% | æ— å¾®è°ƒ |
| Frozen Base + LoRA Head | 51.2% | 51.1% | 99.2% | 67.5% | ä»…åˆ†ç±»å¤´ |

### å…³é”®å‘ç°

1. **Label Scoring æ˜¾è‘—ä¼˜äºä¼ ç»Ÿåˆ†ç±»å¤´**
   - å‡†ç¡®ç‡æå‡ï¼š95.75% vs 93.8% (+1.95%)
   - æ— éœ€é¢å¤–åˆ†ç±»å±‚å‚æ•°ï¼Œè¯­ä¹‰å¯¹é½æ›´è‡ªç„¶
   
2. **å¾®è°ƒæ•ˆæœæ˜¾è‘—**
   - ç›¸æ¯” Zero-shot æå‡ 7.75% å‡†ç¡®ç‡
   - F1 åˆ†æ•°æå‡ 7.6 ä¸ªç™¾åˆ†ç‚¹

3. **LoRA é«˜æ•ˆæ€§éªŒè¯**
   - ä»…å¾®è°ƒ ~1% å‚æ•°å³å¯è¾¾åˆ°ä¼˜å¼‚æ•ˆæœ
   - è®­ç»ƒæ—¶é—´ï¼šçº¦ 56 åˆ†é’Ÿï¼ˆ3 epochsï¼‰

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: Windows ç³»ç»Ÿ bitsandbytes æŠ¥é”™æ€ä¹ˆåŠï¼Ÿ

**é—®é¢˜ç—‡çŠ¶**ï¼š
```
packaging.version.InvalidVersion: Invalid version: '"r"):read("*a"))()'
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å¸è½½åŸç‰ˆ bitsandbytes
pip uninstall bitsandbytes -y

# å®‰è£… Windows å…¼å®¹ç‰ˆæœ¬
pip install bitsandbytes-windows
```

é¡¹ç›®å·²åœ¨ `requirements.txt` ä¸­é…ç½®è‡ªåŠ¨æ£€æµ‹æ“ä½œç³»ç»Ÿå¹¶å®‰è£…å¯¹åº”ç‰ˆæœ¬ï¼Œæ–°ç¯å¢ƒå®‰è£…æ—¶ä¼šè‡ªåŠ¨å¤„ç†ã€‚

### Q2: å®ƒæ¶ˆå¤±äº†

### Q3: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

```bash
# ä½¿ç”¨ QLoRA
python scripts/train_classifier.py --use_qlora

# å‡å°æ‰¹æ¬¡å¤§å°
python scripts/train_classifier.py --batch_size 2

# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
python scripts/train_classifier.py --gradient_checkpointing
```

### Q4: è®­ç»ƒä¸­æ–­äº†æ€ä¹ˆåŠï¼Ÿ

è®­ç»ƒä¼šè‡ªåŠ¨ä¿å­˜ checkpointï¼ˆæ¯100æ­¥ï¼‰ï¼Œå¯ä»¥æ–­ç‚¹ç»­è®­ï¼š

```bash
# è‡ªåŠ¨æ£€æµ‹å¹¶æ¢å¤æœ€æ–°çš„ checkpoint
python scripts/train_classifier.py --resume_from_checkpoint auto

# æ‰‹åŠ¨æŒ‡å®š checkpoint
python scripts/train_classifier.py --resume_from_checkpoint \
    ./experiments/classifier_head/checkpoint-500
```

**æ³¨æ„**ï¼šcheckpoint ä¿å­˜åœ¨ `experiments/*/checkpoint-*` ç›®å½•ï¼Œæœ€å¤šä¿ç•™3ä¸ªæœ€æ–°çš„ã€‚

### Q5: å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®ï¼Ÿ

åœ¨ `data/data_loader.py` ä¸­ä¿®æ”¹ `load_local_dataset` å‡½æ•°ï¼Œå‡†å¤‡ JSON Lines æ ¼å¼æ•°æ®ï¼š

```json
{"text": "è¯„è®ºæ–‡æœ¬", "label": 0}
{"text": "è¯„è®ºæ–‡æœ¬", "label": 1}
```

### Q6: å¦‚ä½•éƒ¨ç½²æ¨¡å‹ï¼Ÿ

```python
from scripts.inference import SentimentPredictor

predictor = SentimentPredictor(
    base_model_name="Qwen/Qwen2.5-1.5B",
    lora_path="./experiments/classifier_head/lora_adapter"
)

result = predictor.predict("è¿™ä¸ªäº§å“å¾ˆå¥½ç”¨ï¼")
print(result["label"])  # æ­£é¢
```

### Q7: ä¸¤ç§æ–¹æ¡ˆå¦‚ä½•é€‰æ‹©ï¼Ÿ

**æ¨èä½¿ç”¨ Label Scoring æ–¹æ¡ˆï¼ˆæ–¹æ¡ˆäºŒï¼‰**ï¼Œå› ä¸ºï¼š
- âœ… å‡†ç¡®ç‡æ›´é«˜ï¼ˆ95.75% vs 93.8%ï¼‰
- âœ… æ— éœ€é¢å¤–åˆ†ç±»å±‚ï¼Œå‚æ•°æ›´å°‘
- âœ… è¯­ä¹‰å¯¹é½æ›´è‡ªç„¶ï¼Œæ³›åŒ–èƒ½åŠ›æ›´å¼º

**ä½¿ç”¨ Classifier Headï¼ˆæ–¹æ¡ˆä¸€ï¼‰çš„åœºæ™¯ï¼š**
- éœ€è¦æè‡´çš„æ¨ç†é€Ÿåº¦
- æ ‡å‡†äºŒåˆ†ç±»ä»»åŠ¡ï¼Œä¸éœ€è¦è¯­ä¹‰å¯¹é½
- ä¸ç°æœ‰ç³»ç»Ÿé›†æˆæ›´æ–¹ä¾¿

## ğŸ“– å­¦ä¹ èµ„æº

- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [QLoRA è®ºæ–‡](https://arxiv.org/abs/2305.14314)
- [PEFT æ–‡æ¡£](https://huggingface.co/docs/peft)
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [Qwen æ¨¡å‹](https://huggingface.co/Qwen)

## ğŸ“„ License

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

---

**Happy Learning! ğŸ‰**
