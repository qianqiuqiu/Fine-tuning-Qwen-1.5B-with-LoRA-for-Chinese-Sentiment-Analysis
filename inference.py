"""
æ¨ç†è„šæœ¬
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿé¢„æµ‹

ä½¿ç”¨æ–¹æ³•:
    python inference.py --model_path ./outputs/lora_adapter --text "è¿™ä¸ªäº§å“éå¸¸å¥½ç”¨ï¼"
    python inference.py --model_path ./outputs/lora_adapter --interactive
"""

import os

# ä¿®å¤ torch å¯¼å…¥å¡æ­»é—®é¢˜ (Intel MKL åº“å†²çª)
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import argparse
import torch
from typing import List, Dict, Any

from transformers import AutoModelForSequenceClassification, AutoTokenizer
from peft import PeftModel


class SentimentPredictor:
    """æƒ…æ„Ÿåˆ†æé¢„æµ‹å™¨"""
    
    def __init__(
        self,
        base_model_name: str = "Qwen/Qwen2.5-1.5B",
        lora_path: str = None,
        device: str = None,
        merge_lora: bool = True,
    ):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            base_model_name: åŸºç¡€æ¨¡å‹åç§°
            lora_path: LoRA é€‚é…å™¨è·¯å¾„ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼‰
            device: è®¾å¤‡ï¼ˆcuda/cpuï¼‰
            merge_lora: æ˜¯å¦åˆå¹¶ LoRA æƒé‡
        """
        
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.labels = ["è´Ÿé¢", "æ­£é¢"]
        
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer_path = lora_path if lora_path else base_model_name
        self.tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_path,
            trust_remote_code=True,
        )
        
        # ç¡®ä¿è®¾ç½® padding token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        # åŠ è½½æ¨¡å‹
        print(f"åŠ è½½æ¨¡å‹: {base_model_name}")
        self.model = AutoModelForSequenceClassification.from_pretrained(
            base_model_name,
            num_labels=2,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
        )
        
        # ç¡®ä¿æ¨¡å‹é…ç½®ä¸­ä¹Ÿè®¾ç½®äº† pad_token_id
        if self.model.config.pad_token_id is None:
            self.model.config.pad_token_id = self.tokenizer.pad_token_id
        
        # åŠ è½½ LoRA
        if lora_path:
            print(f"åŠ è½½ LoRA é€‚é…å™¨: {lora_path}")
            self.model = PeftModel.from_pretrained(self.model, lora_path)
            
            if merge_lora:
                print("åˆå¹¶ LoRA æƒé‡...")
                self.model = self.model.merge_and_unload()
        
        self.model.to(self.device)
        self.model.eval()
        
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼\n")
    
    def predict(
        self,
        text: str,
        max_length: int = 256,
    ) -> Dict[str, Any]:
        """
        å¯¹å•æ¡æ–‡æœ¬è¿›è¡Œé¢„æµ‹
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_length: æœ€å¤§é•¿åº¦
        
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        
        # åˆ†è¯
        inputs = self.tokenizer(
            text,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors="pt",
        ).to(self.device)
        
        # æ¨ç†
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)
            pred_id = torch.argmax(logits, dim=-1).item()
        
        return {
            "text": text,
            "label": self.labels[pred_id],
            "label_id": pred_id,
            "confidence": probs[0][pred_id].item(),
            "probabilities": {
                "è´Ÿé¢": probs[0][0].item(),
                "æ­£é¢": probs[0][1].item(),
            }
        }
    
    def predict_batch(
        self,
        texts: List[str],
        max_length: int = 256,
        batch_size: int = 16,
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡é¢„æµ‹
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            max_length: æœ€å¤§é•¿åº¦
            batch_size: æ‰¹æ¬¡å¤§å°
        
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        
        results = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            # åˆ†è¯
            inputs = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt",
            ).to(self.device)
            
            # æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=-1)
                pred_ids = torch.argmax(logits, dim=-1)
            
            # æ•´ç†ç»“æœ
            for j, text in enumerate(batch_texts):
                pred_id = pred_ids[j].item()
                results.append({
                    "text": text,
                    "label": self.labels[pred_id],
                    "label_id": pred_id,
                    "confidence": probs[j][pred_id].item(),
                    "probabilities": {
                        "è´Ÿé¢": probs[j][0].item(),
                        "æ­£é¢": probs[j][1].item(),
                    }
                })
        
        return results


def interactive_mode(predictor: SentimentPredictor):
    """äº¤äº’å¼é¢„æµ‹æ¨¡å¼"""
    
    print("=" * 50)
    print("äº¤äº’å¼æƒ…æ„Ÿåˆ†æ")
    print("è¾“å…¥æ–‡æœ¬è¿›è¡Œé¢„æµ‹ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("=" * 50)
    
    while True:
        print()
        text = input("è¯·è¾“å…¥æ–‡æœ¬: ").strip()
        
        if text.lower() in ['quit', 'exit', 'q']:
            print("å†è§ï¼")
            break
        
        if not text:
            print("è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")
            continue
        
        result = predictor.predict(text)
        
        print(f"\né¢„æµ‹ç»“æœï¼š")
        print(f"  æƒ…æ„Ÿ: {result['label']}")
        print(f"  ç½®ä¿¡åº¦: {result['confidence']:.2%}")
        print(f"  æ¦‚ç‡åˆ†å¸ƒ: è´Ÿé¢ {result['probabilities']['è´Ÿé¢']:.2%}, "
              f"æ­£é¢ {result['probabilities']['æ­£é¢']:.2%}")


def demo_examples(predictor: SentimentPredictor):
    """æ¼”ç¤ºç¤ºä¾‹"""
    
    examples = [
        # æ­£é¢ç¤ºä¾‹
        "è¿™å®¶é…’åº—ç¯å¢ƒéå¸¸å¥½ï¼ŒæœåŠ¡æ€åº¦ä¹Ÿå¾ˆæ£’ï¼Œä¸‹æ¬¡è¿˜ä¼šæ¥ï¼",
        "äº§å“è´¨é‡å¾ˆå¥½ï¼Œç‰©æµä¹Ÿå¿«ï¼Œéå¸¸æ»¡æ„çš„ä¸€æ¬¡è´­ç‰©ä½“éªŒã€‚",
        "ç”µå½±å‰§æƒ…ç²¾å½©ï¼Œæ¼”å‘˜æ¼”æŠ€åœ¨çº¿ï¼Œå¼ºçƒˆæ¨èï¼",
        "è¿™æœ¬ä¹¦å†™å¾—å¤ªå¥½äº†ï¼Œè®©æˆ‘å—ç›ŠåŒªæµ…ã€‚",
        
        # è´Ÿé¢ç¤ºä¾‹
        "æœåŠ¡æ€åº¦å¤ªå·®äº†ï¼Œå†ä¹Ÿä¸ä¼šæ¥äº†ã€‚",
        "äº§å“è´¨é‡å¾ˆå·®ï¼Œç”¨äº†ä¸¤å¤©å°±åäº†ï¼Œéå¸¸å¤±æœ›ã€‚",
        "ç­‰äº†åŠä¸ªå°æ—¶è¿˜æ²¡ä¸Šèœï¼Œä½“éªŒæå·®ã€‚",
        "è¿™ä¸ªç”µå½±å¤ªæ— èŠäº†ï¼Œæµªè´¹äº†ä¸¤ä¸ªå°æ—¶ã€‚",
        
        # è¾¹ç•Œ/ä¸­æ€§ç¤ºä¾‹
        "è¿˜è¡Œå§ï¼Œä¸€èˆ¬èˆ¬ã€‚",
        "äº§å“è¿˜å¯ä»¥ï¼Œä½†æ˜¯ä»·æ ¼æœ‰ç‚¹è´µã€‚",
    ]
    
    print("=" * 60)
    print("ç¤ºä¾‹é¢„æµ‹")
    print("=" * 60)
    
    results = predictor.predict_batch(examples)
    
    for result in results:
        emoji = "ğŸ˜Š" if result["label"] == "æ­£é¢" else "ğŸ˜"
        print(f"\n{emoji} [{result['label']}] (ç½®ä¿¡åº¦: {result['confidence']:.2%})")
        print(f"   \"{result['text'][:50]}{'...' if len(result['text']) > 50 else ''}\"")


def main(args):
    """ä¸»å‡½æ•°"""
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = SentimentPredictor(
        base_model_name=args.base_model,
        lora_path=args.model_path,
        merge_lora=True,
    )
    
    # è¿è¡Œç¤ºä¾‹
    if args.demo:
        demo_examples(predictor)
    
    # å•æ¡é¢„æµ‹
    if args.text:
        result = predictor.predict(args.text)
        print(f"\nè¾“å…¥: {result['text']}")
        print(f"é¢„æµ‹: {result['label']}")
        print(f"ç½®ä¿¡åº¦: {result['confidence']:.2%}")
    
    # äº¤äº’æ¨¡å¼
    if args.interactive:
        interactive_mode(predictor)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    
    parser = argparse.ArgumentParser(description="æƒ…æ„Ÿåˆ†ææ¨ç†")
    
    parser.add_argument(
        "--model_path",
        type=str,
        default="./outputs/lora_adapter",
        help="LoRA é€‚é…å™¨è·¯å¾„",
    )
    parser.add_argument(
        "--base_model",
        type=str,
        default="Qwen/Qwen2.5-1.5B",
        help="åŸºç¡€æ¨¡å‹åç§°",
    )
    parser.add_argument(
        "--text",
        type=str,
        default=None,
        help="è¦é¢„æµ‹çš„æ–‡æœ¬",
    )
    parser.add_argument(
        "--interactive",
        action="store_true",
        help="äº¤äº’å¼æ¨¡å¼",
    )
    parser.add_argument(
        "--demo",
        action="store_true",
        default=False,
        help="è¿è¡Œç¤ºä¾‹æ¼”ç¤º",
    )
    parser.add_argument(
        "--no-demo",
        dest="demo",
        action="store_false",
        help="ä¸è¿è¡Œç¤ºä¾‹æ¼”ç¤º",
    )
    
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    main(args)
