"""
å¾®è°ƒä¸»è„šæœ¬ï¼ˆæ–¹æ¡ˆäºŒï¼šLoRA + Label Scoring / ç”Ÿæˆå¼åˆ†ç±»ï¼‰
ä½¿ç”¨ LoRA å¾®è°ƒ Qwen-1.5B çš„ CausalLMï¼Œé€šè¿‡æ¯”è¾ƒæ ‡ç­¾ token çš„ log-prob è¿›è¡Œåˆ†ç±»

ä¸ train.pyï¼ˆæ–¹æ¡ˆä¸€ï¼šLoRA + classifier headï¼‰çš„æ ¸å¿ƒåŒºåˆ«ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ train.py (åˆ¤åˆ«å¼åˆ†ç±»å¤´)       â”‚ train2.py (ç”Ÿæˆå¼ Label Scoring)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ AutoModelForSeqCls           â”‚ AutoModelForCausalLM                    â”‚
â”‚ é¢å¤– Linear(dâ†’2) åˆ†ç±»å¤´      â”‚ å¤ç”¨ LM Headï¼ˆè¯è¡¨æŠ•å½±ï¼Œä¸è¯åµŒå…¥å…±äº«æƒé‡ï¼‰  â”‚
â”‚ è¾“å‡º: softmax(Wh+b), 2 ç»´    â”‚ è¾“å‡º: æ¯”è¾ƒ "æ­£é¢"/"è´Ÿé¢" çš„ logprob       â”‚
â”‚ TaskType.SEQ_CLS             â”‚ TaskType.CAUSAL_LM                      â”‚
â”‚ loss = CE(åˆ†ç±» logits, label) â”‚ loss = CE(next-token logits, label_ids) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Prompt æ¨¡æ¿:
    è¾“å…¥: "è¯„è®ºï¼š{text}\næƒ…æ„Ÿå€¾å‘ï¼š"
    æ ‡ç­¾: "æ­£é¢" (label=1) / "è´Ÿé¢" (label=0)

è®­ç»ƒ: å°† prompt + æ ‡ç­¾ æ‹¼æ¥ï¼Œä»…å¯¹æ ‡ç­¾ token è®¡ç®— lossï¼›åªæ›´æ–° LoRA æƒé‡
æ¨ç†: åœ¨ prompt æœ«å°¾æ¯”è¾ƒ "æ­£é¢" ä¸ "è´Ÿé¢" çš„ç”Ÿæˆ log-probï¼Œå–è¾ƒå¤§è€…

ä½¿ç”¨æ–¹æ³•:
    python train2.py                          # ä½¿ç”¨é»˜è®¤é…ç½®
    python train2.py --use_qlora              # ä½¿ç”¨ QLoRAï¼ˆ4-bit é‡åŒ–ï¼‰
    python train2.py --lora_r 16              # è‡ªå®šä¹‰ LoRA ç§©
    python train2.py --num_epochs 5           # è‡ªå®šä¹‰è®­ç»ƒè½®æ•°
"""

import os

# ä¿®å¤ torch å¯¼å…¥å¡æ­»é—®é¢˜ (Intel MKL åº“å†²çª)
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import argparse
import torch
import torch.nn.functional as F
import numpy as np
from typing import Dict, Any, List, Optional, Tuple, Union
import wandb
import glob
import json
from dataclasses import dataclass

# è®¾ç½® HuggingFace é•œåƒï¼ˆç”¨äºåœ¨çº¿ä¸‹è½½æ—¶åŠ é€Ÿï¼‰
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
from datasets import load_dataset, DatasetDict, Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    BitsAndBytesConfig,
    PreTrainedTokenizer,
    DataCollatorForSeq2Seq,
)
from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training

# å¯¼å…¥é¡¹ç›®é…ç½®ï¼ˆå¤ç”¨ training_config ä¸­çš„æ•°æ®ç±»ï¼‰
from configs import (
    ModelConfig,
    DataConfig,
    TrainingConfig,
    QLoRAConfig,
    get_training_args,
)
from data import load_sentiment_dataset

# è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import accuracy_score, precision_recall_fscore_support


# ==================== æ ‡ç­¾å®šä¹‰ ====================

# ä¸­æ–‡æƒ…æ„Ÿæ ‡ç­¾æ˜ å°„
LABEL_TEXTS = {
    0: "è´Ÿé¢",   # negative
    1: "æ­£é¢",   # positive
}

# Prompt æ¨¡æ¿
PROMPT_TEMPLATE = "è¯„è®ºï¼š{text}\næƒ…æ„Ÿå€¾å‘ï¼š"


# ==================== å·¥å…·å‡½æ•° ====================

def get_local_model_path(model_name: str) -> str:
    """
    æ£€æµ‹å¹¶è¿”å›æœ¬åœ°æ¨¡å‹è·¯å¾„

    Args:
        model_name: HuggingFace æ¨¡å‹åç§°ï¼Œå¦‚ "Qwen/Qwen2.5-1.5B"

    Returns:
        æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ–åŸå§‹æ¨¡å‹åç§°
    """
    cache_folder = "models--" + model_name.replace("/", "--")

    if os.path.exists(cache_folder):
        snapshot_pattern = os.path.join(cache_folder, "snapshots", "*")
        snapshots = glob.glob(snapshot_pattern)
        if snapshots:
            model_path = snapshots[0]
            print(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹: {model_path}")
            return model_path

    print(f"ğŸŒ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†ä» HuggingFace ä¸‹è½½: {model_name}")
    return model_name


def get_local_dataset_path(dataset_name: str) -> tuple:
    """
    æ£€æµ‹å¹¶è¿”å›æœ¬åœ°æ•°æ®é›†è·¯å¾„
    """
    cache_folder = "datasets--" + dataset_name.replace("/", "--")

    if os.path.exists(cache_folder):
        snapshot_pattern = os.path.join(cache_folder, "snapshots", "*")
        snapshots = glob.glob(snapshot_pattern)
        if snapshots:
            dataset_path = snapshots[0]
            print(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°æ•°æ®é›†: {dataset_path}")
            return True, dataset_path

    print(f"ğŸŒ æœ¬åœ°æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå°†ä» HuggingFace ä¸‹è½½: {dataset_name}")
    return False, dataset_name


def get_last_checkpoint(output_dir: str) -> Optional[str]:
    """è·å–æœ€æ–°çš„ checkpoint è·¯å¾„"""
    if not os.path.isdir(output_dir):
        return None

    checkpoints = [
        os.path.join(output_dir, d)
        for d in os.listdir(output_dir)
        if d.startswith("checkpoint-") and os.path.isdir(os.path.join(output_dir, d))
    ]

    if not checkpoints:
        return None

    latest_checkpoint = max(checkpoints, key=os.path.getctime)
    return latest_checkpoint


# ==================== æ•°æ®å¤„ç†ï¼ˆLabel Scoring ä¸“ç”¨ï¼‰ ====================

def get_label_token_ids(tokenizer: PreTrainedTokenizer) -> Dict[int, List[int]]:
    """
    è·å–å„æ ‡ç­¾å­—ç¬¦ä¸²å¯¹åº”çš„ token ID åˆ—è¡¨

    Args:
        tokenizer: åˆ†è¯å™¨

    Returns:
        {label_int: [token_id, ...]}  ä¾‹å¦‚ {0: [è´Ÿ, é¢], 1: [æ­£, é¢]}
    """
    label_token_ids = {}
    for label_int, label_text in LABEL_TEXTS.items():
        # ç”¨ encode è·å–çº¯æ–‡æœ¬çš„ token IDï¼ˆä¸åŠ ç‰¹æ®Š tokenï¼‰
        ids = tokenizer.encode(label_text, add_special_tokens=False)
        label_token_ids[label_int] = ids
        print(f"  æ ‡ç­¾ {label_int} ('{label_text}') -> token IDs: {ids}  "
              f"(decoded: {tokenizer.decode(ids)})")
    return label_token_ids


def build_label_scoring_examples(
    examples: Dict[str, List],
    tokenizer: PreTrainedTokenizer,
    max_length: int = 256,
    text_column: str = "text",
    label_column: str = "label",
) -> Dict[str, List]:
    """
    ä¸º Label Scoring æ„å»ºè®­ç»ƒæ ·æœ¬

    æ¯æ¡æ•°æ® -> prompt + label_textï¼Œæ‹¼æ¥ä¸ºå®Œæ•´åºåˆ—
    labels ä¸­ï¼Œprompt éƒ¨åˆ†ç½®ä¸º -100ï¼ˆä¸å‚ä¸ lossï¼‰ï¼Œä»…æ ‡ç­¾ token è®¡ç®— loss

    Args:
        examples: åŸå§‹æ•°æ®æ‰¹æ¬¡
        tokenizer: åˆ†è¯å™¨
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        text_column: æ–‡æœ¬åˆ—å
        label_column: æ ‡ç­¾åˆ—å

    Returns:
        {input_ids, attention_mask, labels}
    """
    all_input_ids = []
    all_attention_mask = []
    all_labels = []

    for text, label in zip(examples[text_column], examples[label_column]):
        # 1) æ„é€  prompt å’Œå®Œæ•´æ–‡æœ¬
        prompt = PROMPT_TEMPLATE.format(text=text)
        label_text = LABEL_TEXTS[label]
        full_text = prompt + label_text

        # 2) åˆ†åˆ« tokenize prompt å’Œå®Œæ•´æ–‡æœ¬
        prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)
        full_ids = tokenizer.encode(full_text, add_special_tokens=False)

        # 3) æˆªæ–­ï¼ˆä» prompt éƒ¨åˆ†æˆªæ–­ï¼Œä¿ç•™æ ‡ç­¾ tokenï¼‰
        label_ids_raw = tokenizer.encode(label_text, add_special_tokens=False)
        label_len = len(label_ids_raw)

        if len(full_ids) > max_length:
            # ä¿ç•™æœ«å°¾çš„æ ‡ç­¾ tokenï¼Œæˆªæ–­ prompt éƒ¨åˆ†
            max_prompt_len = max_length - label_len
            prompt_ids = prompt_ids[:max_prompt_len]
            full_ids = prompt_ids + label_ids_raw

        prompt_len = len(full_ids) - label_len  # é‡æ–°è®¡ç®— prompt é•¿åº¦

        # 4) Padding
        seq_len = len(full_ids)
        pad_len = max_length - seq_len

        input_ids = full_ids + [tokenizer.pad_token_id] * pad_len
        attention_mask = [1] * seq_len + [0] * pad_len

        # 5) æ„é€  labelsï¼šprompt éƒ¨åˆ† = -100ï¼Œæ ‡ç­¾ token ä¿ç•™ï¼Œpadding = -100
        #    Causal LM çš„ label æ˜¯å³ç§»çš„ï¼Œå³ labels[i] æ˜¯ input_ids[i+1] çš„ç›®æ ‡
        #    æ‰€ä»¥ labels é•¿åº¦ä¸ input_ids ç›¸åŒï¼Œå«ä¹‰æ˜¯ position i çš„é¢„æµ‹ç›®æ ‡
        #    HuggingFace CausalLM å†…éƒ¨ä¼šå¤„ç† shiftï¼š
        #      - logits = model(input_ids)   # shape [seq_len, vocab]
        #      - shift_logits = logits[..., :-1, :]
        #      - shift_labels = labels[..., 1:]
        #      - loss = CE(shift_logits, shift_labels)
        #    å› æ­¤ labels[i] åº”è¯¥ç­‰äº input_ids[i]ï¼ˆæ¨¡å‹å†…éƒ¨è´Ÿè´£ shiftï¼‰
        labels = [-100] * prompt_len + full_ids[prompt_len:] + [-100] * pad_len

        all_input_ids.append(input_ids)
        all_attention_mask.append(attention_mask)
        all_labels.append(labels)

    return {
        "input_ids": all_input_ids,
        "attention_mask": all_attention_mask,
        "labels": all_labels,
    }


def create_label_scoring_dataset(
    dataset: DatasetDict,
    tokenizer: PreTrainedTokenizer,
    max_length: int = 256,
    text_column: str = "text",
    label_column: str = "label",
    num_proc: int = 4,
) -> DatasetDict:
    """
    å°†åŸå§‹æ•°æ®é›†è½¬æ¢ä¸º Label Scoring æ ¼å¼

    Args:
        dataset: åŸå§‹ DatasetDict
        tokenizer: åˆ†è¯å™¨
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        text_column / label_column: åˆ—å
        num_proc: å¹¶è¡Œå¤„ç†è¿›ç¨‹æ•°

    Returns:
        å¤„ç†åçš„ DatasetDict
    """

    def transform_fn(examples):
        return build_label_scoring_examples(
            examples=examples,
            tokenizer=tokenizer,
            max_length=max_length,
            text_column=text_column,
            label_column=label_column,
        )

    # è·å–éœ€è¦ç§»é™¤çš„åˆ—
    sample_split = list(dataset.keys())[0]
    columns_to_remove = dataset[sample_split].column_names

    tokenized_dataset = dataset.map(
        transform_fn,
        batched=True,
        num_proc=num_proc,
        remove_columns=columns_to_remove,
        desc="Building label-scoring dataset",
    )

    return tokenized_dataset


# ==================== è‡ªå®šä¹‰ Trainer ====================

class LabelScoringTrainer(Trainer):
    """
    è‡ªå®šä¹‰ Trainerï¼Œä¸º Label Scoring æ–¹æ¡ˆæä¾›ï¼š
    1. æ ‡å‡† Causal LM lossï¼ˆè®­ç»ƒï¼Œåªå¯¹æ ‡ç­¾ token è®¡ç®— lossï¼‰
    2. åŸºäº logprob çš„è¯„ä¼°ï¼ˆæ¯”è¾ƒå€™é€‰æ ‡ç­¾çš„ log-probabilityï¼‰
    """

    def __init__(self, *args, label_token_ids: Dict[int, List[int]] = None,
                 eval_dataset_raw=None, eval_tokenizer=None,
                 eval_max_length: int = 256, **kwargs):
        """
        Args:
            label_token_ids: {label_int: [token_ids]} å„æ ‡ç­¾çš„ token ID
            eval_dataset_raw: åŸå§‹ï¼ˆæœª tokenize çš„ï¼‰éªŒè¯/æµ‹è¯•é›†ï¼Œç”¨äº logprob è¯„ä¼°
            eval_tokenizer: åˆ†è¯å™¨
            eval_max_length: è¯„ä¼°æ—¶çš„æœ€å¤§é•¿åº¦
        """
        super().__init__(*args, **kwargs)
        self.label_token_ids = label_token_ids or {}
        self.eval_dataset_raw = eval_dataset_raw
        self.eval_tokenizer = eval_tokenizer
        self.eval_max_length = eval_max_length

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        """
        æ ‡å‡† Causal LM lossï¼Œlabels ä¸­ -100 çš„ä½ç½®è‡ªåŠ¨è¢«å¿½ç•¥
        """
        outputs = model(**inputs)
        loss = outputs.loss

        return (loss, outputs) if return_outputs else loss

    @torch.no_grad()
    def evaluate(
        self,
        eval_dataset=None,
        ignore_keys=None,
        metric_key_prefix: str = "eval",
    ) -> Dict[str, float]:
        """
        é‡å†™ evaluateï¼šç”¨ label scoringï¼ˆlogprob æ¯”è¾ƒï¼‰åšåˆ†ç±»è¯„ä¼°

        å¯¹éªŒè¯é›†ä¸­çš„æ¯æ¡æ ·æœ¬ï¼š
          1. æ„å»º prompt: "è¯„è®ºï¼š{text}\næƒ…æ„Ÿå€¾å‘ï¼š"
          2. å¯¹äºæ¯ä¸ªå€™é€‰æ ‡ç­¾ï¼Œè®¡ç®—å…¶ token åºåˆ—çš„æ¡ä»¶ log-prob
          3. é€‰æ‹© log-prob æœ€å¤§çš„æ ‡ç­¾ä½œä¸ºé¢„æµ‹
          4. è®¡ç®— accuracy / precision / recall / f1
        """
        model = self.model
        model.eval()
        device = next(model.parameters()).device
        tokenizer = self.eval_tokenizer

        # ç¡®å®šè¯„ä¼°æ•°æ®é›†
        raw_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset_raw
        if raw_dataset is None:
            # å›é€€åˆ°çˆ¶ç±»è¡Œä¸º
            return super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)

        all_preds = []
        all_labels = []

        for sample in raw_dataset:
            text = sample["text"]
            gold_label = sample["label"]

            prompt = PROMPT_TEMPLATE.format(text=text)
            prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)

            # æˆªæ–­ promptï¼ˆä¸ºæ ‡ç­¾ token ç•™ç©ºé—´ï¼‰
            max_label_len = max(len(ids) for ids in self.label_token_ids.values())
            if len(prompt_ids) > self.eval_max_length - max_label_len:
                prompt_ids = prompt_ids[:self.eval_max_length - max_label_len]

            best_label = -1
            best_logprob = float('-inf')

            for label_int, label_ids in self.label_token_ids.items():
                # æ‹¼æ¥ prompt + label
                full_ids = prompt_ids + label_ids
                input_tensor = torch.tensor([full_ids], device=device)
                attention_mask = torch.ones_like(input_tensor)

                outputs = model(input_ids=input_tensor, attention_mask=attention_mask)
                logits = outputs.logits  # [1, seq_len, vocab_size]

                # è®¡ç®—æ ‡ç­¾ token çš„ log-prob
                # logits[t] é¢„æµ‹çš„æ˜¯ position t+1 çš„ token
                # æ ‡ç­¾ token åœ¨ full_ids ä¸­çš„ä½ç½®: prompt_len ~ prompt_len + label_len - 1
                # å¯¹åº”çš„ logits: prompt_len - 1 ~ prompt_len + label_len - 2
                prompt_len = len(prompt_ids)
                log_prob = 0.0
                for i, token_id in enumerate(label_ids):
                    logit_pos = prompt_len - 1 + i
                    log_probs = F.log_softmax(logits[0, logit_pos, :], dim=-1)
                    log_prob += log_probs[token_id].item()

                if log_prob > best_logprob:
                    best_logprob = log_prob
                    best_label = label_int

            all_preds.append(best_label)
            all_labels.append(gold_label)

        # è®¡ç®—æŒ‡æ ‡
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)

        accuracy = accuracy_score(all_labels, all_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_labels, all_preds, average='binary'
        )

        # åŒæ—¶è·å– causal LM lossï¼ˆåœ¨ tokenized eval dataset ä¸Šï¼‰
        loss_metrics = {}
        if self.eval_dataset is not None:
            loss_output = super().evaluate(
                eval_dataset=self.eval_dataset,
                ignore_keys=ignore_keys,
                metric_key_prefix=metric_key_prefix,
            )
            loss_metrics = loss_output

        metrics = {
            f"{metric_key_prefix}_accuracy": accuracy,
            f"{metric_key_prefix}_precision": precision,
            f"{metric_key_prefix}_recall": recall,
            f"{metric_key_prefix}_f1": f1,
        }

        # åˆå¹¶ loss
        if f"{metric_key_prefix}_loss" in loss_metrics:
            metrics[f"{metric_key_prefix}_loss"] = loss_metrics[f"{metric_key_prefix}_loss"]

        # æ—¥å¿—
        self.log(metrics)
        print(f"\n{'='*40} è¯„ä¼°ç»“æœ {'='*40}")
        for k, v in metrics.items():
            print(f"  {k}: {v:.4f}")
        print(f"{'='*90}\n")

        return metrics


# ==================== æ¨¡å‹ & LoRA åˆå§‹åŒ– ====================

def setup_model_and_tokenizer(
    model_config: ModelConfig,
    qlora_config: QLoRAConfig,
) -> tuple:
    """
    åˆå§‹åŒ– CausalLM æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆæ³¨æ„ï¼šä¸ä½¿ç”¨åˆ†ç±»å¤´ï¼‰
    """
    print(f"æ­£åœ¨åŠ è½½ CausalLM æ¨¡å‹: {model_config.model_name_or_path}")

    tokenizer = AutoTokenizer.from_pretrained(
        model_config.model_name_or_path,
        trust_remote_code=model_config.trust_remote_code,
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # é…ç½®é‡åŒ–
    quantization_config = None
    if qlora_config.use_qlora:
        print("å¯ç”¨ QLoRA 4-bit é‡åŒ–...")
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=qlora_config.load_in_4bit,
            bnb_4bit_quant_type=qlora_config.bnb_4bit_quant_type,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=qlora_config.bnb_4bit_use_double_quant,
        )

    # â˜… å…³é”®åŒºåˆ«ï¼šä½¿ç”¨ AutoModelForCausalLMï¼Œä¸ä½¿ç”¨ AutoModelForSequenceClassification
    model = AutoModelForCausalLM.from_pretrained(
        model_config.model_name_or_path,
        trust_remote_code=model_config.trust_remote_code,
        quantization_config=quantization_config,
        device_map="auto" if qlora_config.use_qlora else None,
        torch_dtype=torch.bfloat16,
    )

    model.config.pad_token_id = tokenizer.pad_token_id

    if qlora_config.use_qlora:
        model = prepare_model_for_kbit_training(model)

    return model, tokenizer


def setup_lora(
    model,
    lora_r: int = 8,
    lora_alpha: int = 32,
    lora_dropout: float = 0.1,
    use_qlora: bool = False,
):
    """
    ä¸º CausalLM æ¨¡å‹æ·»åŠ  LoRA é€‚é…å™¨

    â˜… å…³é”®åŒºåˆ«ï¼štask_type=CAUSAL_LMï¼ˆè€Œé SEQ_CLSï¼‰
    """
    print(f"é…ç½® LoRA (CAUSAL_LM): r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}")

    target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ]

    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,  # â˜… ç”Ÿæˆå¼ä»»åŠ¡
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        target_modules=target_modules,
        bias="none",
        inference_mode=False,
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    return model


# ==================== ä¸»è®­ç»ƒå‡½æ•° ====================

def train(args: argparse.Namespace):
    """ä¸»è®­ç»ƒå‡½æ•°ï¼ˆLabel Scoring æ–¹æ¡ˆï¼‰"""

    # ==================== 1. é…ç½®åˆå§‹åŒ– ====================
    local_model_path = get_local_model_path(args.model_name)

    model_config = ModelConfig(
        model_name_or_path=local_model_path,
        num_labels=2,
    )

    data_config = DataConfig(
        dataset_name=args.dataset,
        max_length=args.max_length,
    )

    training_config = TrainingConfig(
        output_dir=args.output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        gradient_checkpointing=args.gradient_checkpointing,
    )

    qlora_config = QLoRAConfig(
        use_qlora=args.use_qlora,
    )

    # ==================== 2. åŠ è½½ CausalLM æ¨¡å‹ ====================
    model, tokenizer = setup_model_and_tokenizer(model_config, qlora_config)

    # ==================== 3. åº”ç”¨ LoRA (CAUSAL_LM) ====================
    model = setup_lora(
        model=model,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        use_qlora=args.use_qlora,
    )

    # ==================== 4. æŸ¥çœ‹æ ‡ç­¾ token æ˜ å°„ ====================
    print("\næ ‡ç­¾ token æ˜ å°„ï¼š")
    label_token_ids = get_label_token_ids(tokenizer)

    # ==================== 5. åŠ è½½å’Œå¤„ç†æ•°æ® ====================
    print(f"\næ­£åœ¨åŠ è½½æ•°æ®é›†: {data_config.dataset_name}")

    use_local, dataset_path = get_local_dataset_path("lansinuote/ChnSentiCorp")
    dataset = load_sentiment_dataset(
        data_config.dataset_name,
        local_path=dataset_path if use_local else None,
    )

    # ä¿å­˜åŸå§‹éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼ˆç”¨äº logprob è¯„ä¼°ï¼‰
    raw_eval_dataset = dataset.get("validation", None)
    raw_test_dataset = dataset.get("test", None)

    # è½¬æ¢ä¸º Label Scoring æ ¼å¼
    print("æ­£åœ¨æ„å»º Label Scoring è®­ç»ƒæ•°æ®...")
    tokenized_dataset = create_label_scoring_dataset(
        dataset=dataset,
        tokenizer=tokenizer,
        max_length=data_config.max_length,
    )

    print(f"è®­ç»ƒé›†å¤§å°: {len(tokenized_dataset['train'])}")
    if "validation" in tokenized_dataset:
        print(f"éªŒè¯é›†å¤§å°: {len(tokenized_dataset['validation'])}")

    # ==================== 6. åˆå§‹åŒ– wandb ====================
    wandb.init(
        project="qwen-sentiment-analysis",
        name=f"label-scoring-lora-r{args.lora_r}-{args.dataset}",
        config={
            "method": "label_scoring",
            "model_name": args.model_name,
            "dataset": args.dataset,
            "lora_r": args.lora_r,
            "lora_alpha": args.lora_alpha,
            "lora_dropout": args.lora_dropout,
            "use_qlora": args.use_qlora,
            "num_epochs": args.num_epochs,
            "batch_size": args.batch_size,
            "learning_rate": args.learning_rate,
            "max_length": args.max_length,
            "label_texts": LABEL_TEXTS,
            "prompt_template": PROMPT_TEMPLATE,
        },
        tags=["LoRA", "label-scoring", "CausalLM",
              "QLoRA" if args.use_qlora else "LoRA", "sentiment-analysis"],
    )

    # ==================== 7. é…ç½®è®­ç»ƒå‚æ•° ====================
    training_args_dict = get_training_args(training_config)

    # Label Scoring æ–¹æ¡ˆçš„ç‰¹æ®Šä¿®æ”¹
    training_args_dict["metric_for_best_model"] = "eval_accuracy"
    training_args_dict["greater_is_better"] = True
    # CausalLM ä¸éœ€è¦ remove_unused_columns=Falseï¼ˆå› ä¸ºè‡ªå®šä¹‰äº†æ•°æ®æ ¼å¼ï¼‰
    training_args_dict["remove_unused_columns"] = False

    training_args = TrainingArguments(**training_args_dict)

    # ==================== 8. åˆå§‹åŒ–è‡ªå®šä¹‰ Trainer ====================
    # DataCollatorï¼šå¤„ç† label shifting çš„ DataCollatorForSeq2Seq ä¸é€‚ç”¨
    # æˆ‘ä»¬å·²ç»åœ¨ build_label_scoring_examples ä¸­æ‰‹åŠ¨å¤„ç†äº† labels
    # å› æ­¤ä½¿ç”¨ default_data_collator
    from transformers import default_data_collator

    trainer = LabelScoringTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset.get("validation", None),
        data_collator=default_data_collator,
        # Label Scoring ä¸“ç”¨å‚æ•°
        label_token_ids=label_token_ids,
        eval_dataset_raw=raw_eval_dataset,
        eval_tokenizer=tokenizer,
        eval_max_length=data_config.max_length,
        callbacks=[
            EarlyStoppingCallback(early_stopping_patience=3),
        ] if args.early_stopping else [],
    )

    # ==================== 9. æ£€æµ‹æ–­ç‚¹ ====================
    checkpoint = None
    if args.resume_from_checkpoint:
        if args.resume_from_checkpoint == "auto":
            checkpoint = get_last_checkpoint(args.output_dir)
            if checkpoint:
                print(f"\næ£€æµ‹åˆ°æ–­ç‚¹: {checkpoint}")
                print("å°†ä»æ–­ç‚¹æ¢å¤è®­ç»ƒ...\n")
            else:
                print("\næœªæ£€æµ‹åˆ°å¯ç”¨çš„ checkpointï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ...\n")
        else:
            checkpoint = args.resume_from_checkpoint
            if os.path.isdir(checkpoint):
                print(f"\nä»æŒ‡å®šæ–­ç‚¹æ¢å¤: {checkpoint}\n")
            else:
                print(f"\nè­¦å‘Š: æŒ‡å®šçš„ checkpoint ä¸å­˜åœ¨: {checkpoint}")
                print("å°†ä»å¤´å¼€å§‹è®­ç»ƒ...\n")
                checkpoint = None

    # ==================== 10. å¼€å§‹è®­ç»ƒ ====================
    print("\n" + "=" * 50)
    print("å¼€å§‹è®­ç»ƒï¼ˆLabel Scoring æ–¹æ¡ˆï¼‰...")
    print("=" * 50 + "\n")

    train_result = trainer.train(resume_from_checkpoint=checkpoint)

    # ==================== 11. ä¿å­˜æ¨¡å‹ ====================
    print("\nä¿å­˜ LoRA é€‚é…å™¨...")

    lora_save_path = os.path.join(args.output_dir, "lora_adapter")
    model.save_pretrained(lora_save_path)
    tokenizer.save_pretrained(lora_save_path)

    # ä¿å­˜æ–¹æ¡ˆå…ƒä¿¡æ¯ï¼ˆæ¨ç†æ—¶éœ€è¦çŸ¥é“ prompt æ¨¡æ¿å’Œæ ‡ç­¾æ˜ å°„ï¼‰
    meta_info = {
        "method": "label_scoring",
        "prompt_template": PROMPT_TEMPLATE,
        "label_texts": {str(k): v for k, v in LABEL_TEXTS.items()},
        "label_token_ids": {str(k): v for k, v in label_token_ids.items()},
        "model_type": "CausalLM",
        "base_model": args.model_name,
    }
    meta_path = os.path.join(lora_save_path, "label_scoring_meta.json")
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta_info, f, ensure_ascii=False, indent=2)
    print(f"å·²ä¿å­˜ Label Scoring å…ƒä¿¡æ¯: {meta_path}")

    trainer.save_metrics("train", train_result.metrics)

    # ==================== 12. æœ€ç»ˆè¯„ä¼°ï¼ˆæµ‹è¯•é›†ï¼‰ ====================
    print("\nåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°ï¼ˆLabel Scoringï¼‰...")
    if raw_test_dataset is not None:
        test_results = trainer.evaluate(
            eval_dataset=raw_test_dataset,
            metric_key_prefix="test",
        )
        print(f"\næµ‹è¯•é›†ç»“æœï¼š")
        for key, value in test_results.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.4f}")
        trainer.save_metrics("test", test_results)

        # è®°å½•æµ‹è¯•ç»“æœåˆ° wandb
        wandb.log({f"test/{key}": value for key, value in test_results.items()
                    if isinstance(value, (int, float))})

    print(f"\nè®­ç»ƒå®Œæˆï¼LoRA é€‚é…å™¨å·²ä¿å­˜åˆ°: {lora_save_path}")

    wandb.finish()

    return trainer


# ==================== å‘½ä»¤è¡Œå‚æ•° ====================

def parse_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""

    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ LoRA + Label Scoring å¾®è°ƒ Qwen è¿›è¡Œä¸­æ–‡æƒ…æ„Ÿåˆ†æ"
    )

    # æ¨¡å‹å‚æ•°
    parser.add_argument(
        "--model_name", type=str, default="Qwen/Qwen2.5-1.5B",
        help="æ¨¡å‹åç§°æˆ–è·¯å¾„",
    )

    # æ•°æ®å‚æ•°
    parser.add_argument(
        "--dataset", type=str, default="ChnSentiCorp",
        choices=["ChnSentiCorp", "IMDB_Chinese"],
        help="æ•°æ®é›†åç§°",
    )
    parser.add_argument(
        "--max_length", type=int, default=256,
        help="æœ€å¤§åºåˆ—é•¿åº¦",
    )

    # LoRA å‚æ•°
    parser.add_argument("--lora_r", type=int, default=8, help="LoRA ç§©")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.1, help="LoRA dropout")
    parser.add_argument("--use_qlora", action="store_true", help="ä½¿ç”¨ QLoRAï¼ˆ4-bit é‡åŒ–ï¼‰")

    # è®­ç»ƒå‚æ•°
    parser.add_argument("--output_dir", type=str, default="./experiments/label_scoring",
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--num_epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=2e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--gradient_checkpointing", action="store_true", default=True,
                        help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
    parser.add_argument("--early_stopping", action="store_true", default=True,
                        help="å¯ç”¨æ—©åœ")
    parser.add_argument(
        "--resume_from_checkpoint", type=str, default="auto",
        help="ä» checkpoint æ¢å¤è®­ç»ƒã€‚'auto' / å…·ä½“è·¯å¾„ / 'none'",
    )

    args = parser.parse_args()

    if args.resume_from_checkpoint.lower() == "none":
        args.resume_from_checkpoint = None

    return args


# ==================== å…¥å£ ====================

if __name__ == "__main__":
    args = parse_args()

    print("=" * 60)
    print("ä¸­æ–‡æƒ…æ„Ÿåˆ†æ - LoRA + Label Scoringï¼ˆç”Ÿæˆå¼åˆ†ç±»ï¼‰")
    print("=" * 60)
    print(f"\né…ç½®ä¿¡æ¯ï¼š")
    print(f"  æ¨¡å‹: {args.model_name}")
    print(f"  æ¨¡å‹ç±»å‹: CausalLMï¼ˆå¤ç”¨ LM Headï¼Œä¸ä½¿ç”¨åˆ†ç±»å¤´ï¼‰")
    print(f"  åˆ†ç±»æ–¹å¼: Label Scoringï¼ˆæ¯”è¾ƒæ ‡ç­¾ logprobï¼‰")
    print(f"  æ ‡ç­¾æ˜ å°„: {LABEL_TEXTS}")
    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  LoRA r: {args.lora_r}")
    print(f"  QLoRA: {'æ˜¯' if args.use_qlora else 'å¦'}")
    print(f"  è®­ç»ƒè½®æ•°: {args.num_epochs}")
    print(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"  è¾“å‡ºç›®å½•: {args.output_dir}")
    print()

    train(args)
