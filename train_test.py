"""
æµ‹è¯•è®­ç»ƒè„šæœ¬
ç”¨äºå¿«é€Ÿæµ‹è¯•è®­ç»ƒæµç¨‹æ˜¯å¦æ­£å¸¸ï¼Œåªä½¿ç”¨50æ¡æ•°æ®

ä½¿ç”¨æ–¹æ³•:
    python train_test.py
    python train_test.py --use_qlora  # æµ‹è¯• QLoRA
"""

import os
import argparse
import torch
import numpy as np
from typing import Dict

from datasets import load_dataset
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig,
)
from peft import get_peft_model, prepare_model_for_kbit_training

# å¯¼å…¥é¡¹ç›®é…ç½®
from configs import (
    get_lora_config,
    ModelConfig,
    DataConfig,
    QLoRAConfig,
)
from data import load_sentiment_dataset, create_tokenized_dataset, get_data_collator

# è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import accuracy_score, precision_recall_fscore_support


def compute_metrics(eval_pred) -> Dict[str, float]:
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    predictions, labels = eval_pred
    
    if len(predictions.shape) > 1:
        predictions = np.argmax(predictions, axis=-1)
    
    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='binary'
    )
    
    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
    }


def setup_model_and_tokenizer(
    model_config: ModelConfig,
    qlora_config: QLoRAConfig,
) -> tuple:
    """åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨"""
    
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_config.model_name_or_path}")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        model_config.model_name_or_path,
        trust_remote_code=model_config.trust_remote_code,
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # é…ç½®é‡åŒ–ï¼ˆQLoRAï¼‰
    quantization_config = None
    if qlora_config.use_qlora:
        print("å¯ç”¨ QLoRA 4-bit é‡åŒ–...")
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=qlora_config.load_in_4bit,
            bnb_4bit_quant_type=qlora_config.bnb_4bit_quant_type,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=qlora_config.bnb_4bit_use_double_quant,
        )
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForSequenceClassification.from_pretrained(
        model_config.model_name_or_path,
        num_labels=model_config.num_labels,
        trust_remote_code=model_config.trust_remote_code,
        quantization_config=quantization_config,
        device_map="auto" if qlora_config.use_qlora else None,
        torch_dtype=torch.bfloat16,
    )
    
    model.config.pad_token_id = tokenizer.pad_token_id
    
    if qlora_config.use_qlora:
        model = prepare_model_for_kbit_training(model)
    
    return model, tokenizer


def setup_lora(model, lora_r: int = 8, lora_alpha: int = 32, lora_dropout: float = 0.1):
    """ä¸ºæ¨¡å‹æ·»åŠ  LoRA é€‚é…å™¨"""
    
    print(f"é…ç½® LoRA: r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}")
    
    lora_config = get_lora_config(
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    return model


def train_test(args: argparse.Namespace):
    """æµ‹è¯•è®­ç»ƒå‡½æ•°"""
    
    print("=" * 60)
    print("æµ‹è¯•è®­ç»ƒæ¨¡å¼ - ä»…ä½¿ç”¨50æ¡æ•°æ®")
    print("=" * 60)
    
    # ==================== 1. é…ç½®åˆå§‹åŒ– ====================
    model_config = ModelConfig(
        model_name_or_path=args.model_name,
        num_labels=2,
    )
    
    data_config = DataConfig(
        dataset_name=args.dataset,
        max_length=256,
    )
    
    qlora_config = QLoRAConfig(
        use_qlora=args.use_qlora,
    )
    
    # ==================== 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ====================
    model, tokenizer = setup_model_and_tokenizer(model_config, qlora_config)
    
    # ==================== 3. åº”ç”¨ LoRA ====================
    model = setup_lora(
        model=model,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
    )
    
    # ==================== 4. åŠ è½½å’Œå¤„ç†æ•°æ® ====================
    print(f"\næ­£åœ¨åŠ è½½æ•°æ®é›†: {data_config.dataset_name}")
    dataset = load_sentiment_dataset(data_config.dataset_name)
    
    print("æ­£åœ¨è¿›è¡Œåˆ†è¯å¤„ç†...")
    tokenized_dataset = create_tokenized_dataset(
        dataset=dataset,
        tokenizer=tokenizer,
        max_length=data_config.max_length,
    )
    
    # ==================== 5. åªé€‰æ‹©å‰50æ¡æ•°æ® ====================
    print("\nâš ï¸  æµ‹è¯•æ¨¡å¼ï¼šåªä½¿ç”¨50æ¡è®­ç»ƒæ•°æ®ï¼Œ20æ¡éªŒè¯æ•°æ®")
    
    train_dataset = tokenized_dataset["train"].select(range(min(50, len(tokenized_dataset["train"]))))
    eval_dataset = tokenized_dataset["validation"].select(range(min(20, len(tokenized_dataset["validation"]))))
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(eval_dataset)}")
    
    # ==================== 6. é…ç½®è®­ç»ƒå‚æ•°ï¼ˆç®€åŒ–ç‰ˆï¼Œæ— é¢„çƒ­ï¼‰ ====================
    # è‡ªåŠ¨æ£€æµ‹ç²¾åº¦æ”¯æŒ
    use_bf16 = False
    use_fp16 = False
    
    if torch.cuda.is_available():
        # æ£€æµ‹æ˜¯å¦æ”¯æŒ bf16
        if torch.cuda.is_bf16_supported():
            use_bf16 = True
            print("âœ… ä½¿ç”¨ BF16 æ··åˆç²¾åº¦è®­ç»ƒ")
        else:
            use_fp16 = True
            print("âœ… ä½¿ç”¨ FP16 æ··åˆç²¾åº¦è®­ç»ƒ (GPU ä¸æ”¯æŒ BF16)")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU è®­ç»ƒ (é€Ÿåº¦è¾ƒæ…¢)")
    
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=8,
        
        # å­¦ä¹ ç‡é…ç½® - æ— é¢„çƒ­
        learning_rate=args.learning_rate,
        warmup_ratio=0.0,  # ä¸ä½¿ç”¨é¢„çƒ­
        lr_scheduler_type="constant",  # ä½¿ç”¨æ’å®šå­¦ä¹ ç‡
        
        # è¯„ä¼°ä¸ä¿å­˜
        eval_strategy="steps",
        eval_steps=10,
        save_strategy="steps",
        save_steps=20,
        save_total_limit=2,
        
        # æ—¥å¿—
        logging_steps=5,
        logging_dir=os.path.join(args.output_dir, "logs"),
        report_to="none",  # ä¸ä½¿ç”¨ tensorboard
        
        # å…¶ä»–
        seed=42,
        bf16=use_bf16,
        fp16=use_fp16,
        dataloader_num_workers=0,  # æµ‹è¯•æ—¶ä¸ä½¿ç”¨å¤šçº¿ç¨‹
        remove_unused_columns=False,
        load_best_model_at_end=False,  # æµ‹è¯•æ—¶ä¸éœ€è¦
    )
    
    # ==================== 7. åˆå§‹åŒ– Trainer ====================
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=get_data_collator(tokenizer),
        compute_metrics=compute_metrics,
    )
    
    # ==================== 8. å¼€å§‹è®­ç»ƒ ====================
    print("\n" + "=" * 50)
    print("å¼€å§‹æµ‹è¯•è®­ç»ƒ...")
    print("=" * 50 + "\n")
    
    train_result = trainer.train()
    
    # ==================== 9. ä¿å­˜æ¨¡å‹ ====================
    print("\nä¿å­˜æµ‹è¯•æ¨¡å‹...")
    
    lora_save_path = os.path.join(args.output_dir, "lora_adapter_test")
    model.save_pretrained(lora_save_path)
    tokenizer.save_pretrained(lora_save_path)
    
    print(f"\nâœ… æµ‹è¯•è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {lora_save_path}")
    print(f"è®­ç»ƒæ­¥æ•°: {train_result.global_step}")
    print(f"æœ€ç»ˆæŸå¤±: {train_result.training_loss:.4f}")
    
    # ==================== 10. å¿«é€Ÿè¯„ä¼° ====================
    print("\nå¿«é€Ÿè¯„ä¼°...")
    eval_results = trainer.evaluate(eval_dataset)
    print(f"\néªŒè¯é›†ç»“æœï¼š")
    for key, value in eval_results.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")
    
    return trainer


def parse_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    
    parser = argparse.ArgumentParser(
        description="æµ‹è¯• LoRA å¾®è°ƒæµç¨‹ï¼ˆ50æ¡æ•°æ®ï¼‰"
    )
    
    parser.add_argument(
        "--model_name",
        type=str,
        default="Qwen/Qwen2.5-1.5B",
        help="æ¨¡å‹åç§°æˆ–è·¯å¾„",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default="ChnSentiCorp",
        help="æ•°æ®é›†åç§°",
    )
    parser.add_argument(
        "--lora_r",
        type=int,
        default=8,
        help="LoRA ç§©",
    )
    parser.add_argument(
        "--lora_alpha",
        type=int,
        default=32,
        help="LoRA alpha",
    )
    parser.add_argument(
        "--lora_dropout",
        type=float,
        default=0.1,
        help="LoRA dropout",
    )
    parser.add_argument(
        "--use_qlora",
        action="store_true",
        help="ä½¿ç”¨ QLoRAï¼ˆ4-bit é‡åŒ–ï¼‰",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./outputs_test",
        help="è¾“å‡ºç›®å½•",
    )
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=2,
        help="è®­ç»ƒè½®æ•°",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=4,
        help="æ‰¹æ¬¡å¤§å°",
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=2e-4,
        help="å­¦ä¹ ç‡",
    )
    
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•è®­ç»ƒæ¨¡å¼")
    print("=" * 60)
    print(f"\né…ç½®ä¿¡æ¯ï¼š")
    print(f"  æ¨¡å‹: {args.model_name}")
    print(f"  æ•°æ®é›†: {args.dataset}")
    print(f"  è®­ç»ƒæ•°æ®: 50 æ¡")
    print(f"  éªŒè¯æ•°æ®: 20 æ¡")
    print(f"  LoRA r: {args.lora_r}")
    print(f"  QLoRA: {'æ˜¯' if args.use_qlora else 'å¦'}")
    print(f"  è®­ç»ƒè½®æ•°: {args.num_epochs}")
    print(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  å­¦ä¹ ç‡: {args.learning_rate} (æ— é¢„çƒ­)")
    print()
    
    train_test(args)
